{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "EudyMnMJpABY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import collections\n",
    "import json\n",
    "import shutil\n",
    "import zipfile\n",
    "import copy\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sentencepiece as spm\n",
    "from tqdm import tqdm\n",
    "\n",
    "random_seed = 1234\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "\n",
    "# tf version 및 gpu 확인\n",
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "print(tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-OntciU4pSFI"
   },
   "source": [
    "# cloud shell\n",
    "\n",
    "$ python\n",
    "\n",
    "import sentencepiece as spm\n",
    "import os\n",
    "corpus_file = os.getenv('HOME')+'/aiffel/bert_pretrain/data/kowiki.txt'\n",
    "prefix = os.getenv('HOME')+'/aiffel/bert_pretrain/models/ko_32000'\n",
    "vocab_size = 32000\n",
    "spm.SentencePieceTrainer.train(f\"--input={corpus_file} --model_prefix={prefix} --vocab_size={vocab_size + 7} --model_type=bpe --max_sentence_length=999999 --pad_id=0 --pad_piece=[PAD] --unk_id=1 --unk_piece=[UNK] --bos_id=2 --bos_piece=[BOS] --eos_id=3 --eos_piece=[EOS] --user_defined_symbols=[SEP],[CLS],[MASK]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "u1UxEsVYpSC5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = os.getenv('HOME')+'/aiffel/bert_pretrain/data'\n",
    "model_dir = os.getenv('HOME')+'/aiffel/bert_pretrain/models'\n",
    "\n",
    "# vocab loading\n",
    "vocab = spm.SentencePieceProcessor()\n",
    "vocab.load(f\"{model_dir}/ko_32000.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "M2R6QVWTpSAs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁1', '▁이', '으로', '에서', '▁있', '▁2', '▁그', '▁대']\n"
     ]
    }
   ],
   "source": [
    "#Q. 특수 token 7개를 제외한 나머지 token들을 출력해봅시다.\n",
    "vocab_list = []\n",
    "for id in range(7, len(vocab)):\n",
    "        if not vocab.is_unknown(id):\n",
    "            # [[YOUR CODE]]\n",
    "            token = vocab.id_to_piece(id)  # 핵심 수정 부분\n",
    "            vocab_list.append(token)\n",
    "\n",
    "print(vocab_list[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "94kBwnzrpR-s"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '▁추적', '추', '적', '▁비가', '▁내리는', '▁날', '이었', '어', '▁그날', '은', '▁', '왠', '지', '▁손', '님이', '▁많아', '▁첫', '▁번에', '▁삼', '십', '▁전', '▁둘째', '번', '▁오', '십', '▁전', '▁오랜', '만에', '▁받아', '보는', '▁십', '▁전', '짜리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손바닥', '▁위', '엔', '▁기쁨', '의', '▁눈', '물이', '▁흘러', '▁컬', '컬', '한', '▁목에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전부터', '▁콜', '록', '거리는', '▁아내', '▁생각에', '▁그', '토록', '▁먹고', '▁싶다', '던', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# [CLS], tokens a, [SEP], tokens b, [SEP] 형태의 token 생성\n",
    "string_a = \"추적추적 비가 내리는 날이었어 그날은 왠지 손님이 많아 첫 번에 삼십 전 둘째번 오십 전 오랜만에 받아보는 십 전짜리 백통화 서푼에\"\n",
    "string_b = \"손바닥 위엔 기쁨의 눈물이 흘러 컬컬한 목에 모주 한잔을 적셔 몇 달 포 전부터 콜록거리는 아내 생각에 그토록 먹고 싶다던\"\n",
    "tokens_org = [\"[CLS]\"] + vocab.encode_as_pieces(string_a) + [\"[SEP]\"] + vocab.encode_as_pieces(string_b) + [\"[SEP]\"]\n",
    "print(tokens_org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "UYt33FuypR8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '▁추적', '추', '적', '▁비가', '▁내리는', '▁날', '이었', '어', '▁그날', '은', '▁', '왠', '지', '▁손', '님이', '▁많아', '▁첫', '▁번에', '▁삼', '십', '▁전', '▁둘째', '번', '▁오', '십', '▁전', '▁오랜', '만에', '▁받아', '보는', '▁십', '▁전', '짜리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손바닥', '▁위', '엔', '▁기쁨', '의', '▁눈', '물이', '▁흘러', '▁컬', '컬', '한', '▁목에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전부터', '▁콜', '록', '거리는', '▁아내', '▁생각에', '▁그', '토록', '▁먹고', '▁싶다', '던', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tokens_org)\n",
    "\n",
    "# 전체 token의 15% mask\n",
    "mask_cnt = int((len(tokens_org) - 3) * 0.15)\n",
    "mask_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3] ['▁추적', '추', '적']\n",
      "[4] ['▁비가']\n",
      "[5] ['▁내리는']\n",
      "[6, 7, 8] ['▁날', '이었', '어']\n",
      "[9, 10] ['▁그날', '은']\n",
      "[11, 12, 13] ['▁', '왠', '지']\n",
      "[14, 15] ['▁손', '님이']\n",
      "[16] ['▁많아']\n",
      "[17] ['▁첫']\n",
      "[18] ['▁번에']\n",
      "[19, 20] ['▁삼', '십']\n",
      "[21] ['▁전']\n",
      "[22, 23] ['▁둘째', '번']\n",
      "[24, 25] ['▁오', '십']\n",
      "[26] ['▁전']\n",
      "[27, 28] ['▁오랜', '만에']\n",
      "[29, 30] ['▁받아', '보는']\n",
      "[31] ['▁십']\n",
      "[32, 33] ['▁전', '짜리']\n",
      "[34, 35, 36] ['▁백', '통', '화']\n",
      "[37, 38, 39] ['▁서', '푼', '에']\n",
      "[41] ['▁손바닥']\n",
      "[42, 43] ['▁위', '엔']\n",
      "[44, 45] ['▁기쁨', '의']\n",
      "[46, 47] ['▁눈', '물이']\n",
      "[48] ['▁흘러']\n",
      "[49, 50, 51] ['▁컬', '컬', '한']\n",
      "[52] ['▁목에']\n",
      "[53, 54] ['▁모', '주']\n",
      "[55, 56, 57] ['▁한', '잔', '을']\n",
      "[58, 59] ['▁적', '셔']\n",
      "[60] ['▁몇']\n",
      "[61] ['▁달']\n",
      "[62] ['▁포']\n",
      "[63] ['▁전부터']\n",
      "[64, 65, 66] ['▁콜', '록', '거리는']\n",
      "[67] ['▁아내']\n",
      "[68] ['▁생각에']\n",
      "[69, 70] ['▁그', '토록']\n",
      "[71] ['▁먹고']\n",
      "[72, 73] ['▁싶다', '던']\n"
     ]
    }
   ],
   "source": [
    "# 띄어쓰기 단위로 mask하기 위해서 index 분할\n",
    "cand_idx = []  # word 단위의 index array\n",
    "for (i, token) in enumerate(tokens_org):\n",
    "    if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "        continue\n",
    "    if 0 < len(cand_idx) and not token.startswith(u\"\\u2581\"):  # u\"\\u2581\"는 단어의 시작을 의미하는 값\n",
    "        cand_idx[-1].append(i)\n",
    "    else:\n",
    "        cand_idx.append([i])\n",
    "\n",
    "# 결과확인\n",
    "for cand in cand_idx:\n",
    "    print(cand, [tokens_org[i] for i in cand])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[18],\n",
       " [44, 45],\n",
       " [24, 25],\n",
       " [49, 50, 51],\n",
       " [31],\n",
       " [63],\n",
       " [41],\n",
       " [52],\n",
       " [22, 23],\n",
       " [71],\n",
       " [17],\n",
       " [19, 20],\n",
       " [60],\n",
       " [32, 33],\n",
       " [62],\n",
       " [46, 47],\n",
       " [29, 30],\n",
       " [72, 73],\n",
       " [6, 7, 8],\n",
       " [64, 65, 66],\n",
       " [67],\n",
       " [9, 10],\n",
       " [26],\n",
       " [55, 56, 57],\n",
       " [61],\n",
       " [34, 35, 36],\n",
       " [37, 38, 39],\n",
       " [21],\n",
       " [58, 59],\n",
       " [48],\n",
       " [69, 70],\n",
       " [4],\n",
       " [27, 28],\n",
       " [42, 43],\n",
       " [14, 15],\n",
       " [68],\n",
       " [5],\n",
       " [11, 12, 13],\n",
       " [1, 2, 3],\n",
       " [16],\n",
       " [53, 54]]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random mask를 위해서 index 순서를 섞음\n",
    "random.shuffle(cand_idx)\n",
    "cand_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens_org\n",
      "['[CLS]', '▁추적', '추', '적', '▁비가', '▁내리는', '▁날', '이었', '어', '▁그날', '은', '▁', '왠', '지', '▁손', '님이', '▁많아', '▁첫', '▁번에', '▁삼', '십', '▁전', '▁둘째', '번', '▁오', '십', '▁전', '▁오랜', '만에', '▁받아', '보는', '▁십', '▁전', '짜리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손바닥', '▁위', '엔', '▁기쁨', '의', '▁눈', '물이', '▁흘러', '▁컬', '컬', '한', '▁목에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전부터', '▁콜', '록', '거리는', '▁아내', '▁생각에', '▁그', '토록', '▁먹고', '▁싶다', '던', '[SEP]'] \n",
      "\n",
      "tokens\n",
      "['[CLS]', '▁추적', '추', '적', '▁비가', '▁내리는', '▁날', '이었', '어', '▁그날', '은', '▁', '왠', '지', '▁손', '님이', '▁많아', '▁첫', '[MASK]', '▁삼', '십', '▁전', '▁둘째', '번', '[MASK]', '[MASK]', '▁전', '▁오랜', '만에', '▁받아', '보는', '▁그레이트', '▁전', '짜리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손바닥', '▁위', '엔', '[MASK]', '[MASK]', '▁눈', '물이', '▁흘러', '[MASK]', '[MASK]', '[MASK]', '▁목에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전부터', '▁콜', '록', '거리는', '▁아내', '▁생각에', '▁그', '토록', '▁먹고', '▁싶다', '던', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# tokens가 mask되므로 재 실행을 위해서 넣어줌 (테스트용)\n",
    "tokens = copy.deepcopy(tokens_org)\n",
    "\n",
    "mask_lms = []  # mask 된 값\n",
    "for index_set in cand_idx:\n",
    "    if len(mask_lms) >= mask_cnt:  # 핸재 mask된 개수가 15%를 넘으면 중지\n",
    "          break\n",
    "    if len(mask_lms) + len(index_set) > mask_cnt:  # 이번에 mask할 개수를 포함해 15%를 넘으면 skip\n",
    "          continue\n",
    "    dice = random.random()  # 0과 1 사이의 확률 값\n",
    "\n",
    "    for index in index_set:\n",
    "        masked_token = None\n",
    "        if dice < 0.8:  # 80% replace with [MASK]\n",
    "            masked_token = \"[MASK]\"\n",
    "        elif dice < 0.9: # 10% keep original\n",
    "            masked_token = tokens[index]\n",
    "        else:  # 10% random word\n",
    "            masked_token = random.choice(vocab_list)\n",
    "        mask_lms.append({\"index\": index, \"label\": tokens[index]})\n",
    "        tokens[index] = masked_token\n",
    "\n",
    "print(\"tokens_org\")\n",
    "print(tokens_org, \"\\n\")\n",
    "print(\"tokens\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask_idx   : [18, 24, 25, 31, 44, 45, 49, 50, 51, 63]\n",
      "mask_label : ['▁번에', '▁오', '십', '▁십', '▁기쁨', '의', '▁컬', '컬', '한', '▁전부터']\n"
     ]
    }
   ],
   "source": [
    "# 순서 정렬 및 mask_idx, mask_label 생성\n",
    "mask_lms = sorted(mask_lms, key=lambda x: x[\"index\"])\n",
    "mask_idx = [p[\"index\"] for p in mask_lms]\n",
    "mask_label = [p[\"label\"] for p in mask_lms]\n",
    "\n",
    "print(\"mask_idx   :\", mask_idx)\n",
    "print(\"mask_label :\", mask_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "SBD-yL5HpRxc"
   },
   "outputs": [],
   "source": [
    "# Q. 위 코드들을 참고하여 아래 함수를 완성시켜주세요.\n",
    "def create_pretrain_mask(tokens, mask_cnt, vocab_list):\n",
    "    \"\"\"\n",
    "    마스크 생성\n",
    "    :param tokens: tokens\n",
    "    :param mask_cnt: mask 개수 (전체 tokens의 15%)\n",
    "    :param vocab_list: vocab list (random token 용)\n",
    "    :return tokens: mask된 tokens\n",
    "    :return mask_idx: mask된 token의 index\n",
    "    :return mask_label: mask된 token의 원래 값\n",
    "    \"\"\"\n",
    "\n",
    "    # 단어 단위로 mask 하기 위해서 index 분할 (띄어쓰기)\n",
    "    # 띄어쓰기 단위로 mask하기 위해서 index 분할\n",
    "    cand_idx = []  # word 단위의 index array\n",
    "    for (i, token) in enumerate(tokens):\n",
    "        if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "            continue\n",
    "        if 0 < len(cand_idx) and not token.startswith(u\"\\u2581\"):  # u\"\\u2581\"는 단어의 시작을 의미하는 값\n",
    "            cand_idx[-1].append(i)\n",
    "        else:\n",
    "            cand_idx.append([i])\n",
    "\n",
    "    # random mask를 위해서 순서를 섞음 (shuffle)\n",
    "    # random mask를 위해서 index 순서를 섞음\n",
    "    random.shuffle(cand_idx)\n",
    "\n",
    "    # mask_lms 정렬 후 mask_idx, mask_label 추출 (sorted 사용)\n",
    "    # tokens가 mask되므로 재 실행을 위해서 넣어줌 (테스트용)\n",
    "    tokens = copy.deepcopy(tokens)\n",
    "\n",
    "    mask_lms = []  # mask 된 값\n",
    "    for index_set in cand_idx:\n",
    "        if len(mask_lms) >= mask_cnt:  # 핸재 mask된 개수가 15%를 넘으면 중지\n",
    "            break\n",
    "        if len(mask_lms) + len(index_set) > mask_cnt:  # 이번에 mask할 개수를 포함해 15%를 넘으면 skip\n",
    "            continue\n",
    "        dice = random.random()  # 0과 1 사이의 확률 값\n",
    "\n",
    "        for index in index_set:\n",
    "            masked_token = None\n",
    "            if dice < 0.8:  # 80% replace with [MASK]\n",
    "                masked_token = \"[MASK]\"\n",
    "            elif dice < 0.9: # 10% keep original\n",
    "                masked_token = tokens[index]\n",
    "            else:  # 10% random word\n",
    "                masked_token = random.choice(vocab_list)\n",
    "            mask_lms.append({\"index\": index, \"label\": tokens[index]})\n",
    "            tokens[index] = masked_token\n",
    "\n",
    "    mask_lms = sorted(mask_lms, key=lambda x: x[\"index\"])\n",
    "    mask_idx = [p[\"index\"] for p in mask_lms]\n",
    "    mask_label = [p[\"label\"] for p in mask_lms]\n",
    "\n",
    "    return tokens, mask_idx, mask_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "pHQlXmSQpRvP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens_org\n",
      "['[CLS]', '▁추적', '추', '적', '▁비가', '▁내리는', '▁날', '이었', '어', '▁그날', '은', '▁', '왠', '지', '▁손', '님이', '▁많아', '▁첫', '▁번에', '▁삼', '십', '▁전', '▁둘째', '번', '▁오', '십', '▁전', '▁오랜', '만에', '▁받아', '보는', '▁십', '▁전', '짜리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손바닥', '▁위', '엔', '▁기쁨', '의', '▁눈', '물이', '▁흘러', '▁컬', '컬', '한', '▁목에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전부터', '▁콜', '록', '거리는', '▁아내', '▁생각에', '▁그', '토록', '▁먹고', '▁싶다', '던', '[SEP]'] \n",
      "\n",
      "tokens\n",
      "['[CLS]', '▁추적', '추', '적', '▁비가', '▁내리는', '▁날', '이었', '어', '▁그날', '은', '▁', '왠', '지', '▁손', '님이', '[MASK]', '[MASK]', '▁번에', '▁삼', '십', '▁전', '▁둘째', '번', '▁오', '십', '▁전', '▁오랜', '만에', '▁받아', '보는', '▁십', '▁전', '짜리', '▁백', '통', '화', '[MASK]', '[MASK]', '[MASK]', '[SEP]', '▁손바닥', '▁위', '엔', '▁기쁨', '의', '▁눈', '물이', '[MASK]', '▁컬', '컬', '한', '[MASK]', '끊', '▁규장', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전부터', '▁콜', '록', '거리는', '[MASK]', '▁생각에', '▁그', '토록', '▁먹고', '▁싶다', '던', '[SEP]'] \n",
      "\n",
      "mask_idx   : [16, 17, 37, 38, 39, 48, 52, 53, 54, 67]\n",
      "mask_label : ['▁많아', '▁첫', '▁서', '푼', '에', '▁흘러', '▁목에', '▁모', '주', '▁아내']\n"
     ]
    }
   ],
   "source": [
    "# tokens가 mask되므로 재 실행을 위해서 넣어줌 (테스트용)\n",
    "tokens = copy.deepcopy(tokens_org)\n",
    "\n",
    "tokens, mask_idx, mask_label = create_pretrain_mask(tokens, mask_cnt, vocab_list)\n",
    "\n",
    "print(\"tokens_org\")\n",
    "print(tokens_org, \"\\n\")\n",
    "print(\"tokens\")\n",
    "print(tokens, \"\\n\")\n",
    "\n",
    "print(\"mask_idx   :\", mask_idx)\n",
    "print(\"mask_label :\", mask_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "1fkNs3k_pRs_"
   },
   "outputs": [],
   "source": [
    "string = \"\"\"추적추적 비가 내리는 날이었어\n",
    "그날은 왠지 손님이 많아\n",
    "첫 번에 삼십 전 둘째 번 오십 전\n",
    "오랜만에 받아보는 십 전짜리 백통화 서푼에\n",
    "손바닥 위엔 기쁨의 눈물이 흘러\n",
    "컬컬한 목에 모주 한잔을 적셔\n",
    "몇 달 포 전부터 콜록거리는 아내\n",
    "생각에 그토록 먹고 싶다던\n",
    "설렁탕 한 그릇을 이제는 살 수 있어\n",
    "집으로 돌아가는 길 난 문득 떠올라\n",
    "아내의 목소리가 거칠어만 가는 희박한 숨소리가\n",
    "오늘은 왠지 나가지 말라던 내 옆에 있어 달라던\n",
    "그리도 나가고 싶으면 일찍이라도 들어와 달라던\n",
    "아내의 간절한 목소리가 들려와\n",
    "나를 원망하듯 비는 점점 거세져\n",
    "싸늘히 식어가는 아내가 떠올라 걱정은 더해져\n",
    "난 몰라 오늘은 운수 좋은 날\n",
    "난 맨날 이렇게 살 수 있으면 얼마나 좋을까\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "dNl_ae31pRq1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['▁추적', '추', '적', '▁비가', '▁내리는', '▁날', '이었', '어'],\n",
       " ['▁그날', '은', '▁', '왠', '지', '▁손', '님이', '▁많아'],\n",
       " ['▁첫', '▁번에', '▁삼', '십', '▁전', '▁둘째', '▁번', '▁오', '십', '▁전']]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 줄 단위로 tokenize\n",
    "doc = [vocab.encode_as_pieces(line) for line in string.split(\"\\n\")]\n",
    "doc[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "NOAtE1TVpRoe"
   },
   "outputs": [],
   "source": [
    "# 최대 길이\n",
    "n_test_seq = 64\n",
    "# 최소 길이\n",
    "min_seq = 8\n",
    "# [CLS], tokens_a, [SEB], tokens_b, [SEP]\n",
    "max_seq = n_test_seq - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "SXtB9bu_pRkN"
   },
   "outputs": [],
   "source": [
    "def trim_tokens(tokens_a, tokens_b, max_seq):\n",
    "    \"\"\"\n",
    "    tokens_a, tokens_b의 길이를 줄임 최대 길이: max_seq\n",
    "    :param tokens_a: tokens A\n",
    "    :param tokens_b: tokens B\n",
    "    :param max_seq: 두 tokens 길이의 최대 값\n",
    "    \"\"\"\n",
    "\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_seq:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a = tokens_a[:-1]\n",
    "        else:\n",
    "            tokens_b = tokens_b[:-1]\n",
    "    return tokens_a, tokens_b\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "NOU-ard2pRdL"
   },
   "outputs": [],
   "source": [
    "# Q. 위 코드들을 참고하여 아래 함수를 완성시켜주세요.\n",
    "def create_pretrain_instances(vocab, doc, n_seq, mask_prob, vocab_list):\n",
    "    max_seq = n_seq - 3\n",
    "    instances = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    \n",
    "    for i in range(len(doc)):\n",
    "        current_chunk.append(doc[i])\n",
    "        current_length += len(doc[i])\n",
    "        \n",
    "        if 1 < len(current_chunk) and (i == len(doc) - 1 or current_length >= max_seq):\n",
    "            a_end = 1 if len(current_chunk) == 2 else random.randrange(1, len(current_chunk))\n",
    "            tokens_a = [token for j in range(a_end) for token in current_chunk[j]]\n",
    "            tokens_b = [token for j in range(a_end, len(current_chunk)) for token in current_chunk[j]]\n",
    "\n",
    "            if random.random() < 0.5:\n",
    "                is_next = 0\n",
    "                tokens_a, tokens_b = tokens_b, tokens_a\n",
    "            else:\n",
    "                is_next = 1\n",
    "            \n",
    "            tokens_a, tokens_b = trim_tokens(tokens_a, tokens_b, max_seq)\n",
    "            if len(tokens_a) == 0 or len(tokens_b) == 0:\n",
    "                continue  # 유효하지 않은 인스턴스는 건너뜁니다.\n",
    "\n",
    "            tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"] + tokens_b + [\"[SEP]\"]\n",
    "            segment = [0] * (len(tokens_a) + 2) + [1] * (len(tokens_b) + 1)\n",
    "        \n",
    "            tokens, mask_idx, mask_label = create_pretrain_mask(tokens, int((len(tokens) - 3) * mask_prob), vocab_list)\n",
    "\n",
    "            instance = {\n",
    "                \"tokens\": tokens,\n",
    "                \"segment\": segment,\n",
    "                \"is_next\": is_next,\n",
    "                \"mask_idx\": mask_idx,\n",
    "                \"mask_label\": mask_label\n",
    "            }\n",
    "            instances.append(instance)\n",
    "\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "        \n",
    "    return instances\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "-sEKM12CpRbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['[CLS]', '선이', '▁번에', '▁삼', '십', '▁전', '▁둘째', '▁번', '▁오', '십', '▁전', '▁오랜', '만에', '▁받아', '보는', '[MASK]', '▁전', '짜리', '▁백', '통', '화', '▁서', '푼', '에', '[MASK]', '▁위', '엔', '▁기쁨', '의', '▁눈', '물이', '▁흘러', '▁컬', '컬', '한', '트리올', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁휘는', '[SEP]', '▁추적', '추', '적', '▁비가', '▁내리는', '▁날', '이었', '어', '▁그날', '은', '[MASK]', '[MASK]', '[MASK]', '▁손', '님이', '▁많아', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [1, 2, 15, 24, 35, 45, 57, 58, 59], 'mask_label': ['▁첫', '▁번에', '▁십', '▁손바닥', '▁목에', '▁포', '▁', '왠', '지']}\n",
      "{'tokens': ['[CLS]', '▁아내의', '▁간', '절한', '▁목소리가', '[MASK]', '[MASK]', '[SEP]', '▁생각에', '▁그', '토록', '▁먹고', '▁싶다', '던', '▁설', '렁', '탕', '▁한', '▁그릇', '을', '▁이제는', '▁살', '▁수', '[MASK]', '▁집으로', '▁돌아가는', '▁길', '▁난', '[MASK]', '[MASK]', '▁떠올', '라', '▁아내의', '▁목소리가', '▁거칠', '어', '만', '▁가는', '▁희', '박한', '[MASK]', '[MASK]', '[MASK]', '▁오늘', '은', '▁', '왠', '지', '▁나가지', '▁말라', '던', '[MASK]', '▁옆에', '▁있어', '▁달라', '던', '▁그리', '도', '▁나가', '고', '▁싶', '으면', '▁일찍', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [5, 6, 23, 28, 29, 40, 41, 42, 51], 'mask_label': ['▁들려', '와', '▁있어', '▁문', '득', '▁숨', '소', '리가', '▁내']}\n",
      "{'tokens': ['[CLS]', '[MASK]', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있으면', '▁얼마나', '[MASK]', '[MASK]', '[MASK]', '[SEP]', '▁나를', '▁원', '망', '하', '듯', '▁비는', '▁점점', '▁거세', '져', '▁싸', '늘', '히', '▁식', '어', '가는', '▁아내가', '▁떠올', '라', '▁걱', '정은', '▁더', '해져', '▁난', '▁몰', '라', '▁오늘', '은', '▁운수', '▁좋은', '[MASK]', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [1, 9, 10, 11, 40, 42], 'mask_label': ['▁난', '▁좋', '을', '까', '▁운수', '▁날']}\n"
     ]
    }
   ],
   "source": [
    "instances = create_pretrain_instances(vocab, doc, n_test_seq, 0.15, vocab_list)\n",
    "\n",
    "# 최종 데이터셋 결과 확인\n",
    "for instance in instances:\n",
    "    print(instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DWkSjR68Newt"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "b_QCdv7IpRZG"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3957761"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_file = os.getenv('HOME')+'/aiffel/bert_pretrain/data/kowiki.txt'\n",
    "\n",
    "# line count 확인\n",
    "total = 0\n",
    "with open(corpus_file, 'r') as in_f:\n",
    "    for line in in_f:\n",
    "        total += 1\n",
    "\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "9cBseqq6pRXM"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 105/3957761 [00:00<11:11, 5895.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 lines : ['▁지미', '▁카터']\n",
      "['▁제임스', '▁얼', '▁\"', '지', '미', '\"', '▁카터', '▁주니어', '(,', '▁1924', '년', '▁10', '월', '▁1', '일', '▁~', '▁)', '는', '▁민주당', '▁출신', '▁미국', '▁39', '번째', '▁대통령', '▁(19', '77', '년', '▁~', '▁1981', '년', ')', '이다', '.']\n",
      "['▁그는', '▁2002', '년', '▁말', '▁인권', '과', '▁중재', '▁역할에', '▁대한', '▁공로를', '▁인정받아', '▁노벨', '▁평화', '상을', '▁받게', '▁되었다', '.']\n",
      "\n",
      "14 lines : ['▁수학']\n",
      "['▁수학', '(', '數', '學', ',', '▁)', '은', '▁양', ',', '▁구조', ',', '▁공간', ',', '▁변화', ',', '▁미', '적', '분', '▁등의', '▁개념을', '▁다루는', '▁학문이다', '.', '▁현대', '▁수학', '은', '▁형식', '▁논', '리를', '▁이용해서', '▁공', '리로', '▁구성된', '▁추상', '적', '▁구조를', '▁연구하는', '▁학문', '으로', '▁여겨', '지기도', '▁한다', '.', '▁수학', '은', '▁그', '▁구조와', '▁발전', '▁과정', '에서는', '▁자연', '과학', '에', '▁속하는', '▁물리', '학을', '▁비롯한', '▁다른', '▁학문', '들과', '▁깊은', '▁연', '관을', '▁맺고', '▁있다', '.', '▁하지만', ',', '▁어느', '▁과학의', '▁분야', '들과는', '▁달리', ',', '▁자연', '계에서', '▁관측', '되지', '▁않는', '▁개념', '들에', '▁대해서', '까지', '▁이론을', '▁일반화', '▁및', '▁추상', '화', '시킬', '▁수', '▁있다는', '▁차이가', '▁있다고', '▁한다', '.', '▁수', '학자들은', '▁그러한', '▁개념', '들에', '▁대해서', '▁추측', '을', '▁하고', ',', '▁적절', '하게', '▁선택', '된', '▁정의', '와', '▁공리', '로부터의', '▁엄', '밀한', '▁연', '역을', '▁통해서', '▁추측', '들의', '▁진', '위를', '▁파악', '한다', '.']\n",
      "['▁수', '학의', '▁기초를', '▁확실히', '▁세우', '기', '▁위해', ',', '▁수리', '논', '리', '학과', '▁집합', '론이', '▁발전', '하였고', ',', '▁이와', '▁더불어', '▁범주', '론이', '▁최근', '에도', '▁발전', '되고', '▁있다', '.', '▁“', '근', '본', '▁위기', '”', '라는', '▁말은', '▁대략', '▁1900', '년에서', '▁1930', '년', '▁사이에', '▁일어난', ',', '▁수', '학의', '▁엄', '밀한', '▁기초', '에', '▁대한', '▁탐', '구를', '▁상징', '적으로', '▁보여주는', '▁말이다', '.', '▁수', '학의', '▁엄', '밀한', '▁기초', '에', '▁대한', '▁몇', '▁가지', '▁의견', '▁불', '일', '치는', '▁오늘날에도', '▁계속되고', '▁있다', '.', '▁수', '학의', '▁기초', '에', '▁대한', '▁위', '기는', '▁그', '▁당시', '▁수많은', '▁논쟁', '에', '▁의해', '▁촉발', '되었으며', ',', '▁그', '▁논쟁', '에는', '▁칸', '토', '어의', '▁집합', '론과', '▁브라우', '어', '-', '힐', '베르트', '▁논쟁이', '▁포함되었다', '.']\n",
      "\n",
      "4 lines : ['▁수학', '▁상수']\n",
      "['▁수학에서', '▁상수', '란', '▁그', '▁값이', '▁변하지', '▁않는', '▁불변', '량으로', ',', '▁변', '수의', '▁반대', '말', '이다', '.', '▁물리', '▁상수', '와는', '▁달리', ',', '▁수학', '▁상', '수는', '▁물리적', '▁측정', '과는', '▁상관없이', '▁정의된다', '.']\n",
      "['▁특정', '▁수학', '▁상수', ',', '▁예를', '▁들면', '▁골', '롬', '-', '딕', '맨', '▁상수', ',', '▁프랑', '세', '즈', '-', '로', '빈', '슨', '▁상수', ',', '▁formula', '_1', ',', '▁레', '비', '▁상수', '같은', '▁상', '수는', '▁다른', '▁수학', '상수', '▁또는', '▁함수', '와', '▁약한', '▁상관', '관계', '▁또는', '▁강한', '▁상관', '관계를', '▁갖는다', '.']\n",
      "\n",
      "10 lines : ['▁문학']\n",
      "['▁문학', '(', '文', '學', ')', '은', '▁언어를', '▁예술적', '▁표현의', '▁제', '재로', '▁삼아', '▁새로운', '▁의미를', '▁창출', '하여', ',', '▁인간과', '▁사회를', '▁진실', '되게', '▁묘사', '하는', '▁예술의', '▁하위', '분야', '이다', '.', '▁간단하게', '▁설명', '하면', ',', '▁언어를', '▁통해', '▁인간의', '▁삶을', '▁미', '적', '(', '美', '的', ')', '으로', '▁형상', '화한', '▁것이라고', '▁볼', '▁수', '▁있다', '.', '▁문학', '은', '▁원래', '▁문예', '(', '文', '藝', ')', '라고', '▁부르는', '▁것이', '▁옳', '으며', ',', '▁문학을', '▁학문의', '▁대상', '으로서', '▁탐구', '하는', '▁학문의', '▁명칭', '▁역시', '▁문예', '학', '이다', '.', '▁문예', '학은', '▁음악', '사', '학', ',', '▁미술', '사', '학', '▁등과', '▁함께', '▁예술', '학의', '▁핵심', '분야', '로서', '▁인문', '학의', '▁하위', '범', '주에', '▁포함된다', '.']\n",
      "['▁반영', '론적', '▁관', '점에', '▁의한', '▁감', '상은', '▁작품을', '▁창작', '된', '▁당시', '▁시대', '▁정', '황', '과', '▁연결', '시켜', '▁감상', '하는', '▁입장', '이고', ',', '▁내재', '적', '▁관', '점의', '▁감', '상은', '▁작품의', '▁형식', ',', '▁내용에', '▁국한', '하여', '▁감상', '하는', '▁것이다', '.', '▁표현', '론적', '▁관', '점의', '▁감', '상은', '▁작가의', '▁전기', '적', '▁사실과', '▁작품을', '▁연결', '시켜', '▁감상', '하는', '▁것이고', ',', '▁수용', '론적', '▁관', '점의', '▁감', '상은', '▁독', '자와', '▁작품을', '▁연결', '시켜', '▁감상', '하는', '▁것을', '▁말한다', '.']\n",
      "\n",
      "10 lines : ['▁나라', '▁목록']\n",
      "['▁이', '▁문서는', '▁나라', '▁목록', '이며', ',', '▁전', '▁세계', '▁20', '6', '개', '▁나라의', '▁각', '▁현황', '과', '▁주권', '▁승인', '▁정보를', '▁개', '요', '▁형태로', '▁나열', '하고', '▁있다', '.']\n",
      "['▁위', '▁목록에', '▁포함되지', '▁않은', '▁다음', '▁국가는', '▁몬테', '비', '데오', '▁협약', '의', '▁모든', '▁조건을', '▁만족', '하지', '▁못', '하거나', ',', '▁자주', '적이고', '▁독립', '적', '임을', '▁주장', '하지', '▁않는', '▁국가이다', '.']\n",
      "\n",
      "['▁화학']\n",
      "['▁화학', '(', '化', '學', ',', '▁)', '은', '▁물질의', '▁성질', ',', '▁조성', ',', '▁구조', ',', '▁변화', '▁및', '▁그에', '▁수반', '하는', '▁에너지의', '▁변화를', '▁연구하는', '▁자연과', '학의', '▁한', '▁분야이다', '.', '▁물리학', '도', '▁역시', '▁물질을', '▁다루는', '▁학문', '이지만', ',', '▁물리학', '이', '▁원', '소와', '▁화합', '물을', '▁모두', '▁포함한', '▁물체의', '▁운동과', '▁에너지', ',', '▁열', '적', '·', '전기', '적', '·', '광', '학적', '·', '기계', '적', '▁속', '성을', '▁다루고', '▁이러한', '▁현상', '으로부터', '▁통일된', '▁이론을', '▁구축', '하려는', '▁것과는', '▁달리', '▁화학', '에서는', '▁물질', '▁자체를', '▁연구', '▁대상으로', '▁한다', '.', '▁화학', '은', '▁이미', '▁존재하는', '▁물질을', '▁이용하여', '▁특정한', '▁목적에', '▁맞는', '▁새로운', '▁물질을', '▁합성', '하는', '▁길을', '▁제공하며', ',', '▁이는', '▁농작', '물의', '▁증', '산', ',', '▁질병의', '▁치료', '▁및', '▁예방', ',', '▁에너지', '▁효율', '▁증대', ',', '▁환경', '오', '염', '▁감소', '▁등', '▁여러', '▁가지', '▁이', '점을', '▁제공한다', '.']\n",
      "['▁유기', '화', '학은', '▁탄', '소로', '▁이루어진', '▁화합', '물을', '▁연구하는', '▁분', '과', '이다', '.', '▁원래', '▁유기', '▁화합', '물은', '▁식물', '이나', '▁동물', '로부터', '▁추출', '해', '낸', '▁화합', '물을', '▁뜻', '하였으나', '▁지금은', '▁유기', '▁화합', '물의', '▁범위가', '▁크게', '▁넓', '어져', '▁탄소', '▁사슬', '▁또는', '▁탄소', '▁고', '리를', '▁가진', '▁모든', '▁화합', '물을', '▁뜻한다', '.', '▁유기', '화', '학의', '▁오랜', '▁관심', '사는', '▁유기', '▁화합', '물의', '▁합성', '▁메커니즘', '이다', '.', '▁현대에', '▁들어서', '▁핵', '자기', '▁공명', '법과', '▁X', '선', '▁결정', '학', '▁등이', '▁개발되어', '▁유기', '▁화합물', '▁분석', '에', '▁있어서', '▁매우', '▁중요한', '▁방법으로', '▁자리잡았다', '.', '▁플라스틱', ',', '▁합성', '섬유', '등의', '▁고분', '자', '물질', '▁등도', '▁유기', '화', '학에서', '▁다루', '어진다', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 위키가 주제별로 잘 나눠지는지 여부 확인\n",
    "count = 5\n",
    "\n",
    "with open(corpus_file, 'r') as in_f:\n",
    "    doc = []  # 단락 단위로 문서 저장\n",
    "    for line in tqdm(in_f, total=total):\n",
    "        line = line.strip()\n",
    "        if line == \"\":  # line이 빈줄 일 경우 (새로운 단락)\n",
    "            if 0 < len(doc):\n",
    "                if 0 < count:\n",
    "                    count -= 1\n",
    "                    print(len(doc), \"lines :\", doc[0])\n",
    "                    print(doc[1])\n",
    "                    print(doc[-1])\n",
    "                    print()\n",
    "                else:\n",
    "                    break\n",
    "                doc = []\n",
    "        else:  # 빈 줄이 아니면 doc에 저장\n",
    "            pieces = vocab.encode_as_pieces(line)\n",
    "            if 0 < len(pieces):\n",
    "                doc.append(pieces)\n",
    "    if 0 < len(doc):  # 마지막에 처리되지 않은 doc가 있는 경우\n",
    "        print(doc[0])\n",
    "        print(doc[1])\n",
    "        print(doc[-1])\n",
    "        doc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "bA4dXWK6pRVN"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 105/3957761 [00:00<07:40, 8590.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc: 21 instances: 10\n",
      "{'tokens': ['[CLS]', '▁유기', '화', '학은', '[MASK]', '[MASK]', '▁이루어진', '老', '奎', '▁연구하는', '▁분', '과', '이다', '.', '▁원래', '▁유기', '▁화합', '물은', '▁식물', '이나', '▁동물', '로부터', '▁추출', '해', '낸', '▁화합', '물을', '▁뜻', '하였으나', '[MASK]', '▁유기', '▁화합', '[SEP]', '▁유기', '화', '학은', '▁탄', '소로', '▁이루어진', '▁화합', '물을', '▁연구하는', '▁분', '과', '이다', '.', '▁원래', '▁유기', '[MASK]', '[MASK]', '▁식물', '이나', '▁동물', '로부터', '▁추출', '해', '낸', '▁화합', '물을', '▁뜻', '하였으나', '▁지금은', '[MASK]', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [4, 5, 7, 8, 9, 29, 48, 49, 62], 'mask_label': ['▁탄', '소로', '▁화합', '물을', '▁연구하는', '▁지금은', '▁화합', '물은', '▁유기']}\n",
      "{'tokens': ['[CLS]', '▁유기', '화', '학은', '▁탄', '소로', '▁이루어진', '▁화합', '물을', '▁연구하는', '▁분', '과', '이다', '.', '▁원래', '▁유기', '▁화합', '물은', '[MASK]', '[MASK]', '▁동물', '로부터', '▁추출', '해', '낸', '▁화합', '물을', '▁뜻', '하였으나', '▁지금은', '▁유기', '▁화합', '[SEP]', '▁유기', '화', '학은', '▁탄', '소로', '▁이루어진', '▁화합', '물을', '▁연구하는', '▁분', '과', '이다', '.', '▁원래', '▁유기', '▁화합', '물은', '▁식물', '이나', '▁동물', '로부터', '▁추출', '해', '낸', '▁화합', '물을', '▁뜻', '하였으나', '▁지금은', '▁유기', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [4, 5, 18, 19, 31, 42, 43, 44, 45], 'mask_label': ['▁탄', '소로', '▁식물', '이나', '▁화합', '▁분', '과', '이다', '.']}\n",
      "\n",
      "doc: 14 instances: 7\n",
      "{'tokens': ['[CLS]', '▁유기', '화', '학은', '▁탄', '소로', '▁다리', '▁화합', '물을', '▁연구하는', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁유기', '▁화합', '물은', '▁식물', '이나', '▁동물', '로부터', '▁추출', '해', '낸', '▁화합', '물을', '▁뜻', '하였으나', '▁지금은', '▁유기', '▁화합', '[SEP]', '▁유기', '화', '학은', '▁탄', '소로', '▁이루어진', '▁화합', '물을', '▁연구하는', '▁분', '과', '이다', '.', '▁원래', '▁유기', '▁화합', '물은', '▁식물', '이나', 'En', '▁로열', '▁추출', '해', '낸', '▁화합', '물을', '▁뜻', '하였으나', '▁지금은', '▁유기', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [6, 10, 11, 12, 13, 14, 29, 52, 53], 'mask_label': ['▁이루어진', '▁분', '과', '이다', '.', '▁원래', '▁지금은', '▁동물', '로부터']}\n",
      "{'tokens': ['[CLS]', '▁유기', '화', '학은', '▁탄', '소로', '[MASK]', '▁화합', '물을', '필을', '▁분', '과', '이다', '.', '▁원래', '▁남편', '▁화합', '물은', '▁식물', '이나', '▁동물', '로부터', '▁추출', '해', '낸', '▁화합', '물을', '▁뜻', '하였으나', '▁지금은', '▁유기', '▁화합', '[SEP]', '▁유기', '화', '학은', '▁탄', '소로', '▁이루어진', '▁화합', '물을', '[MASK]', '▁분', '과', '이다', '.', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁식물', '이나', '▁동물', '로부터', '▁추출', '해', '낸', '▁화합', '물을', '▁뜻', '하였으나', '[MASK]', '▁유기', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [6, 9, 15, 41, 46, 47, 48, 49, 61], 'mask_label': ['▁이루어진', '▁연구하는', '▁유기', '▁연구하는', '▁원래', '▁유기', '▁화합', '물은', '▁지금은']}\n",
      "\n",
      "doc: 4 instances: 2\n",
      "{'tokens': ['[CLS]', '▁유기', '화', '학은', '▁탄', '소로', '[MASK]', '▁화합', '물을', '▁연구하는', '▁분', '과', '이다', '.', '▁원래', '▁유기', '▁화합', '물은', '▁식물', '이나', '▁동물', '로부터', '▁추출', '해', '낸', '▁화합', '물을', '[MASK]', '[MASK]', '▁지금은', '[MASK]', '▁화합', '[SEP]', '[MASK]', '[MASK]', '[MASK]', '▁탄', '소로', '▁이루어진', '▁화합', '물을', '[MASK]', '▁분', '과', '이다', '.', '▁원래', '[MASK]', '▁화합', '물은', '▁식물', '이나', '▁동물', '로부터', '▁추출', '해', '낸', '▁화합', '물을', '▁뜻', '하였으나', '▁지금은', '▁유기', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [6, 27, 28, 30, 33, 34, 35, 41, 47], 'mask_label': ['▁이루어진', '▁뜻', '하였으나', '▁유기', '▁유기', '화', '학은', '▁연구하는', '▁유기']}\n",
      "{'tokens': ['[CLS]', '▁유기', '화', '학은', '[MASK]', '[MASK]', '▁이루어진', '▁화합', '물을', '▁연구하는', '▁분', '과', '이다', '.', '▁원래', '▁유기', '▁화합', '물은', '▁식물', '이나', '▁동물', '로부터', '▁추출', '해', '낸', '▁화합', '물을', '▁뜻', '하였으나', '[MASK]', '[MASK]', '▁화합', '[SEP]', '▁유기', '화', '학은', '▁탄', '소로', '▁이루어진', '▁화합', '물을', '[MASK]', '▁분', '과', '이다', '.', '▁원래', '▁유기', '▁화합', '물은', '▁식물', '이나', '▁동물', '로부터', '▁추출', '해', '낸', '▁화합', '물을', '▁뜻', '하였으나', '▁지금은', '▁유기', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [4, 5, 29, 30, 41, 46, 54, 55, 56], 'mask_label': ['▁탄', '소로', '▁지금은', '▁유기', '▁연구하는', '▁원래', '▁추출', '해', '낸']}\n",
      "\n",
      "doc: 10 instances: 5\n",
      "{'tokens': ['[CLS]', '[MASK]', '[MASK]', '[MASK]', '▁탄', '소로', '▁이루어진', '[MASK]', '[MASK]', '▁연구하는', '▁분', '과', '이다', '.', '▁원래', '▁유기', '▁화합', '물은', '▁식물', '이나', '▁동물', '로부터', '▁추출', '해', '낸', '▁화합', '물을', '▁뜻', '하였으나', '▁지금은', '[MASK]', '[MASK]', '[SEP]', '▁유기', '화', '학은', '▁탄', '소로', '▁이루어진', '▁화합', '물을', '▁연구하는', '▁분', '과', '이다', '.', '▁원래', '▁유기', '[MASK]', '[MASK]', '▁식물', '이나', '▁동물', '로부터', '▁추출', '해', '낸', '▁화합', '물을', '▁뜻', '하였으나', '▁지금은', '▁유기', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [1, 2, 3, 7, 8, 30, 31, 48, 49], 'mask_label': ['▁유기', '화', '학은', '▁화합', '물을', '▁유기', '▁화합', '▁화합', '물은']}\n",
      "{'tokens': ['[CLS]', '▁유기', '화', '학은', '▁탄', '소로', '萬', '▁화합', '물을', '▁연구하는', '▁분', '과', '이다', '.', '[MASK]', '▁유기', '▁화합', '물은', '▁식물', '이나', '▁동물', '로부터', '▁추출', '해', '낸', '[MASK]', '[MASK]', '▁뜻', '하였으나', '▁지금은', '▁유기', '▁화합', '[SEP]', '▁유기', '화', '학은', '▁탄', '소로', '[MASK]', '▁모르고', '▁시작되었으며', '▁연구하는', '▁분', '과', '이다', '.', '▁원래', '▁유기', '▁화합', '물은', '▁식물', '이나', '▁동물', '로부터', '▁추출', '해', '낸', '▁화합', '물을', '▁뜻', '하였으나', '[MASK]', '▁유기', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [6, 14, 25, 26, 38, 39, 40, 61, 62], 'mask_label': ['▁이루어진', '▁원래', '▁화합', '물을', '▁이루어진', '▁화합', '물을', '▁지금은', '▁유기']}\n",
      "\n",
      "doc: 10 instances: 5\n",
      "{'tokens': ['[CLS]', '[MASK]', '[MASK]', '[MASK]', '▁탄', '소로', '▁이루어진', '▁화합', '물을', '▁연구하는', '▁분', '과', '이다', '.', '▁원래', '▁유기', '▁화합', '물은', '▁식물', '이나', '▁동물', '로부터', '▁추출', '해', '낸', '▁화합', '물을', '▁뜻', '하였으나', '▁지금은', '▁유기', '▁화합', '[SEP]', '▁유기', '화', '학은', '▁탄', '소로', '[MASK]', '▁화합', '물을', '▁연구하는', '▁분', '과', '이다', '.', '[MASK]', '▁유기', '▁위헌', '▁조직', '▁식물', '이나', '▁동물', '로부터', '▁추출', '해', '낸', '[MASK]', '[MASK]', '▁뜻', '하였으나', '▁지금은', '▁유기', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [1, 2, 3, 38, 46, 48, 49, 57, 58], 'mask_label': ['▁유기', '화', '학은', '▁이루어진', '▁원래', '▁화합', '물은', '▁화합', '물을']}\n",
      "{'tokens': ['[CLS]', '▁유기', '화', '학은', '▁탄', '소로', '▁이루어진', '▁화합', '물을', '▁연구하는', '▁분', '과', '이다', '.', '▁원래', '▁유기', '▁화합', '물은', '▁식물', '이나', '▁동물', '로부터', '▁추출', '해', '낸', '▁화합', '물을', '[MASK]', '[MASK]', '[MASK]', '▁유기', '▁화합', '[SEP]', '[MASK]', '[MASK]', '[MASK]', '▁탄', '소로', '▁이루어진', '▁화합', '물을', '▁연구하는', '▁분', '과', '이다', '.', '[MASK]', '▁유기', '▁화합', '물은', '玉', '왠', '▁동물', '로부터', '▁추출', '해', '낸', '▁화합', '물을', '▁뜻', '하였으나', '▁지금은', '▁유기', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [27, 28, 29, 33, 34, 35, 46, 50, 51], 'mask_label': ['▁뜻', '하였으나', '▁지금은', '▁유기', '화', '학은', '▁원래', '▁식물', '이나']}\n",
      "\n",
      "doc: 31 instances: 15\n",
      "{'tokens': ['[CLS]', '▁유기', '화', '학은', '▁탄', '소로', '▁이루어진', '[MASK]', '[MASK]', '▁연구하는', '▁분', '과', '이다', '.', '▁원래', '▁유기', '▁화합', '물은', '▁식물', '이나', '▁동물', '로부터', '▁추출', '해', '낸', '▁화합', '물을', '[MASK]', '[MASK]', '▁지금은', '▁유기', '[MASK]', '[SEP]', '▁유기', '화', '학은', '▁탄', '소로', '[MASK]', '▁화합', '물을', '▁연구하는', '▁분', '과', '이다', '.', '▁원래', '▁유기', '▁화합', '물은', '[MASK]', '[MASK]', '▁동물', '로부터', '▁추출', '해', '낸', '▁화합', '물을', '▁뜻', '하였으나', '[MASK]', '▁유기', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [7, 8, 27, 28, 31, 38, 50, 51, 61], 'mask_label': ['▁화합', '물을', '▁뜻', '하였으나', '▁화합', '▁이루어진', '▁식물', '이나', '▁지금은']}\n",
      "{'tokens': ['[CLS]', '▁유기', '화', '학은', '▁탄', '소로', '▁이루어진', '▁화합', '물을', '▁연구하는', '▁분', '과', '이다', '.', '▁원래', '▁유기', '▁화합', '물은', '▁식물', '이나', '▁동물', '로부터', '▁추출', '해', '낸', '▁화합', '물을', '▁뜻', '하였으나', '▁지금은', '▁유기', '▁화합', '[SEP]', '▁유기', '화', '학은', '▁탄', '소로', '[MASK]', '▁화합', '물을', '▁연구하는', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁원래', '▁유기', '▁화합', '물은', '▁식물', '이나', '[MASK]', '[MASK]', '▁추출', '해', '낸', '[MASK]', '[MASK]', '▁뜻', '하였으나', '▁지금은', '▁유기', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [38, 42, 43, 44, 45, 52, 53, 57, 58], 'mask_label': ['▁이루어진', '▁분', '과', '이다', '.', '▁동물', '로부터', '▁화합', '물을']}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# instance 생성 기능 확인\n",
    "count = 5\n",
    "\n",
    "with open(corpus_file, 'r') as in_f:\n",
    "    doc = []  # 단락 단위로 문서 저장\n",
    "    for line in tqdm(in_f, total=total):\n",
    "        line = line.strip()\n",
    "        if line == \"\":  # line이 빈줄 일 경우 (새로운 단락)\n",
    "            if 0 < len(doc):\n",
    "                instances = create_pretrain_instances(vocab, doc, n_test_seq, 0.15, vocab_list)\n",
    "                # save\n",
    "                print(\"doc:\", len(doc), \"instances:\", len(instances))\n",
    "                print(instances[0])\n",
    "                print(instances[-1])\n",
    "                print()\n",
    "                doc = []\n",
    "                if 0 < count:  # 테스트를 위해서 부분 처리함\n",
    "                    count -= 1\n",
    "                else:\n",
    "                    break\n",
    "        else:  # doc에 저장\n",
    "            if 0 < len(pieces):\n",
    "                doc.append(pieces)\n",
    "    if 0 < len(doc):  # 마지막에 처리되지 않은 doc가 있는 경우\n",
    "        instances = create_pretrain_instances(doc, 128)\n",
    "        # save\n",
    "        print(\"doc:\", len(doc), \"instances:\", len(instances))\n",
    "        print(instances[0])\n",
    "        print(instances[-1])\n",
    "        print()\n",
    "        doc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "Fn5kFLTJpRS9"
   },
   "outputs": [],
   "source": [
    "# Q. 아래 주석에 따라 코드를 완성해주세요.\n",
    "def make_pretrain_data(vocab, in_file, out_file, n_seq, mask_prob=0.15):\n",
    "    \"\"\" pretrain 데이터 생성 \"\"\"\n",
    "    def save_pretrain_instances(out_f, doc):\n",
    "        instances = create_pretrain_instances(vocab, doc, n_seq, mask_prob, vocab_list)\n",
    "        for instance in instances:\n",
    "            out_f.write(json.dumps(instance, ensure_ascii=False))\n",
    "            out_f.write(\"\\n\")\n",
    "\n",
    "    # 특수문자 7개를 제외한 vocab_list 생성\n",
    "    vocab_list = []\n",
    "    for id in range(7, len(vocab)):\n",
    "        if not vocab.is_unknown(id):\n",
    "            vocab_list.append(vocab.id_to_piece(id))\n",
    "\n",
    "    # line count 확인\n",
    "    line_cnt = 0\n",
    "    with open(in_file, \"r\") as in_f:\n",
    "        for line in in_f:\n",
    "            line_cnt += 1\n",
    "\n",
    "    with open(in_file, \"r\") as in_f:\n",
    "        with open(out_file, \"w\") as out_f:\n",
    "            doc = []\n",
    "            for line in tqdm(in_f, total=line_cnt):\n",
    "                line = line.strip()\n",
    "                if line == \"\":  # line이 빈줄 일 경우 (새로운 단락)\n",
    "                    if doc:\n",
    "                        save_pretrain_instances(out_f, doc)\n",
    "                    doc = []\n",
    "                else:  # line이 빈줄이 아닐 경우 tokenize 해서 doc에 저장\n",
    "                    tokens = vocab.encode_as_pieces(line)\n",
    "                    doc.append(tokens)\n",
    "            if doc:  # 마지막에 처리되지 않은 doc가 있는 경우\n",
    "                save_pretrain_instances(out_f, doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pretrain_instances(vocab, doc, n_seq, mask_prob, vocab_list):\n",
    "    max_seq = n_seq - 3\n",
    "    instances = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    \n",
    "    for i in range(len(doc)):\n",
    "        current_chunk.append(doc[i])\n",
    "        current_length += len(doc[i])\n",
    "        \n",
    "        if 1 < len(current_chunk) and (i == len(doc) - 1 or current_length >= max_seq):\n",
    "            a_end = 1 if len(current_chunk) == 2 else random.randrange(1, len(current_chunk))\n",
    "            tokens_a = [token for j in range(a_end) for token in current_chunk[j]]\n",
    "            tokens_b = [token for j in range(a_end, len(current_chunk)) for token in current_chunk[j]]\n",
    "\n",
    "            if random.random() < 0.5:\n",
    "                is_next = 0\n",
    "                tokens_a, tokens_b = tokens_b, tokens_a\n",
    "            else:\n",
    "                is_next = 1\n",
    "            \n",
    "            tokens_a, tokens_b = trim_tokens(tokens_a, tokens_b, max_seq)\n",
    "            if len(tokens_a) == 0 or len(tokens_b) == 0:\n",
    "                continue  # 유효하지 않은 인스턴스는 건너뜁니다.\n",
    "\n",
    "            tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"] + tokens_b + [\"[SEP]\"]\n",
    "            segment = [0] * (len(tokens_a) + 2) + [1] * (len(tokens_b) + 1)\n",
    "        \n",
    "            tokens, mask_idx, mask_label = create_pretrain_mask(tokens, int((len(tokens) - 3) * mask_prob), vocab_list)\n",
    "\n",
    "            instance = {\n",
    "                \"tokens\": tokens,\n",
    "                \"segment\": segment,\n",
    "                \"is_next\": is_next,\n",
    "                \"mask_idx\": mask_idx,\n",
    "                \"mask_label\": mask_label\n",
    "            }\n",
    "            instances.append(instance)\n",
    "\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "        \n",
    "    return instances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "VqS0B91ApRRG",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3957761/3957761 [06:53<00:00, 9571.99it/s] \n"
     ]
    }
   ],
   "source": [
    "pretrain_json_path = os.getenv('HOME')+'/aiffel/bert_pretrain/data/bert_pre_train.json'\n",
    "\n",
    "make_pretrain_data(vocab, corpus_file, pretrain_json_path, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3957761"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_file = os.getenv('HOME')+'/aiffel/bert_pretrain/data/kowiki.txt'\n",
    "\n",
    "# line count 확인\n",
    "total = 0\n",
    "with open(corpus_file, 'r') as in_f:\n",
    "    for line in in_f:\n",
    "        total += 1\n",
    "\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "S0hiQBENpRO_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "862250"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 라인수\n",
    "total = 0\n",
    "with open(pretrain_json_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        total += 1\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "Opsom2a1hFPH"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " 0,\n",
       " 0,\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_seq = 128\n",
    "# [CLS], tokens_a, [SEP], tokens_b, [SEP]\n",
    "max_seq = n_seq - 3\n",
    "\n",
    "# 만약 일반적인 Numpy Array에다 데이터를 로딩한다면 이렇게 되겠지만\n",
    "# enc_tokens = np.zeros((total, n_seq), np.int32)\n",
    "# dec_tokens = np.zeros((total, n_seq), np.int32)\n",
    "# labels_nsp = np.zeros((total,), np.int32)\n",
    "# labels_mlm = np.zeros((total, n_seq), np.int32)\n",
    "\n",
    "# np.memmap을 사용하면 메모리를 적은 메모리에서도 대용량 데이터 처리가 가능 함\n",
    "enc_tokens = np.memmap(filename='enc_tokens.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "segments = np.memmap(filename='segments.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "labels_nsp = np.memmap(filename='labels_nsp.memmap', mode='w+', dtype=np.int32, shape=(total,))\n",
    "labels_mlm = np.memmap(filename='labels_mlm.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "\n",
    "\n",
    "enc_tokens[0], enc_tokens[-1], segments[0], segments[-1], labels_nsp[0], labels_nsp[-1], labels_mlm[0], labels_mlm[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "rj61UXSphFNI"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/862250 [00:00<?, ?it/s]/tmp/ipykernel_31/767648317.py:16: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  mask_idx = np.array(data[\"mask_idx\"], dtype=np.int)\n",
      "/tmp/ipykernel_31/767648317.py:17: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  mask_label = np.array([vocab.piece_to_id(p) for p in data[\"mask_label\"]], dtype=np.int)\n",
      "/tmp/ipykernel_31/767648317.py:18: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  label_mlm = np.full(n_seq, dtype=np.int, fill_value=0)\n",
      "  0%|          | 6/862250 [00:00<35:19, 406.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['[CLS]', '▁지미', '▁카터', '▁제임스', '▁얼', '▁\"', '지', '미', '\"', '[MASK]', '[MASK]', '[MASK]', '▁1924', '년', '껑', 'ζ', '▁1', '일', '▁~', '▁)', '는', '▁민주당', '▁출신', '[MASK]', '▁39', '번째', '[MASK]', '▁(19', '77', '년', '▁~', '▁1981', '년', ')', '이다', '.', '▁지미', '▁카', '터는', '[MASK]', '[MASK]', '▁섬', '터', '[MASK]', '▁플', '레인', '스', '▁마을에서', '▁태어났다', '.', '[MASK]', '▁공과', '대학교를', '▁졸업하였다', '.', '▁그', '▁후', '▁해군에', '▁들어가', '▁전함', '·', '원자', '력', '·', '[SEP]', '▁1962', '년', '▁조지아', '▁주', '▁상원', '[MASK]', '▁선거에서', '▁낙선', '하나', '▁그', '▁이름은', '▁부정', '선거', '▁', '였', '음을', '▁입증', '하게', '▁되어', '[MASK]', '[MASK]', '[MASK]', '▁1966', '년', '▁조지아', '▁주', '▁지사', '▁선거에', '▁낙선', '하지만', '▁1970', '년', '▁조지아', '▁주', '▁지', '사를', '▁역임했다', '.', 'arg', '▁되기', '▁전', '▁조지아', '주', '▁상원의', '원을', '▁두', '번', '▁연', '임', '했으며', ',', '▁1971', '년부터', '▁1975', '년까지', '[MASK]', '▁지', '사로', '▁근무했다', '.', '▁조지아', '▁주지', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [9, 10, 11, 14, 15, 23, 26, 39, 40, 43, 50, 70, 75, 84, 85, 86, 103, 120], 'mask_label': ['▁카터', '▁주니어', '(,', '▁10', '월', '▁미국', '▁대통령', '▁조지아', '주', '▁카운티', '▁조지아', '▁의원', '▁선거가', '▁당선', '되고', ',', '▁대통령이', '▁조지아']}\n",
      "enc_token: [5, 16415, 25250, 3324, 1042, 103, 27610, 27686, 27718, 6, 6, 6, 5708, 27625, 29622, 31759, 7, 27629, 203, 241, 27602, 4867, 788, 6, 5898, 796, 6, 1647, 4630, 27625, 203, 3008, 27625, 27616, 16, 27599, 16415, 207, 4612, 6, 6, 630, 27714, 6, 429, 5346, 27626, 14406, 1605, 27599, 6, 14146, 15991, 8637, 27599, 13, 81, 25987, 2247, 15033, 27873, 14475, 27813, 27873, 4, 3715, 27625, 5551, 37, 11234, 6, 5249, 9858, 3294, 13, 1636, 2386, 2163, 27596, 27671, 969, 8047, 173, 607, 6, 6, 6, 3926, 27625, 5551, 37, 18995, 8198, 9858, 1447, 1921, 27625, 5551, 37, 18, 451, 4267, 27599, 25536, 6436, 25, 5551, 27646, 18205, 928, 157, 27821, 61, 27773, 530, 27604, 3372, 523, 3409, 673, 6, 18, 982, 13264, 27599, 5551, 5053, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 1\n",
      "label_mlm: [    0     0     0     0     0     0     0     0     0 25250  7504   416\n",
      "     0     0   131 27662     0     0     0     0     0     0     0   243\n",
      "     0     0   663     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0  5551 27646     0     0  4269     0     0     0     0\n",
      "     0     0  5551     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0  2378     0\n",
      "     0     0     0 20590     0     0     0     0     0     0     0     0\n",
      "  2387   317 27604     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0  4864     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "  5551     0     0     0     0     0     0     0]\n",
      "\n",
      "{'tokens': ['[CLS]', '▁그러나', '▁이것은', '[MASK]', '[MASK]', '▁미국의', '▁유대인', '▁단체의', '▁반발을', '▁일으켰다', '.', '▁1979', '년', '▁백악', '관에서', '▁양국', '▁간의', '▁평화', '조약', '으로', '▁이끌', '어졌다', '.', '▁또한', '▁소련과', '[MASK]', '[MASK]', '[MASK]', '▁전략', '▁무기', '▁제한', '▁협', '상에', '▁조인', '했다', '.', '▁카', '터는', '▁1970', '년대', '▁후반', '▁당시', '▁대한민국', '▁등', '[MASK]', '▁후진', '국의', '▁국민들의', '▁인', '권을', '▁지키기', '▁위해', '▁노력', '했으며', ',', '▁취임', '▁이후', '▁계속해서', '▁도덕', '정', '치를', '▁내세', '웠다', '.', '[SEP]', '▁1976', '년', '[MASK]', '▁선거에', '▁민주당', '▁후보로', '▁출마하여', '▁도덕', '주의', '▁정책으로', '▁내세워', ',', '▁포', '드를', '▁누르고', '▁당선되었다', '.', '▁카터', '▁대통령은', '▁에너지', '▁개발을', '▁촉구', '했으나', '▁공화', '당의', '▁반대로', '[MASK]', '[MASK]', '▁카', '터는', '▁이집', '트와', '▁이스라엘', '을', '▁조정', '하여', ',', '▁캠프', '▁데이비', '드에서', '[MASK]', '[MASK]', '[MASK]', '▁사다', '트', '▁대통령과', '▁메', '나', '헴', '▁베', '긴', '▁수상', '과', '▁함께', '▁중동', '▁평화를', '[MASK]', '▁캠프', '데이', '비', '드', '▁협정을', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [3, 4, 25, 26, 27, 43, 44, 67, 91, 92, 105, 106, 107, 114, 115, 116, 117, 121], 'mask_label': ['▁공화', '당과', '▁제', '2', '차', '▁등', '▁인권', '▁대통령', '▁무산되었다', '.', '▁안', '와', '르', '▁베', '긴', '▁수상', '과', '▁위한']}\n",
      "enc_token: [5, 330, 1487, 6, 6, 679, 7455, 15747, 21408, 6564, 27599, 2995, 27625, 10312, 6749, 13195, 2714, 2793, 8993, 9, 1435, 2521, 27599, 276, 23197, 6, 6, 6, 2835, 3841, 1956, 617, 1824, 15876, 31, 27599, 207, 4612, 1921, 596, 1840, 316, 410, 50, 6, 17092, 137, 18896, 42, 917, 15177, 231, 3375, 530, 27604, 2659, 165, 6357, 6244, 27642, 1233, 5890, 1853, 27599, 4, 3306, 27625, 6, 8198, 4867, 4896, 19160, 6244, 238, 22033, 19990, 27604, 119, 1486, 10071, 7965, 27599, 25250, 5906, 3634, 8085, 9747, 1003, 4460, 1547, 4771, 6, 6, 207, 4612, 2703, 3604, 3426, 27607, 3358, 54, 27604, 10251, 3640, 3552, 6, 6, 6, 7025, 27677, 13799, 334, 27637, 29887, 271, 28099, 1011, 27644, 280, 8021, 14237, 6, 10251, 4282, 27694, 27681, 15990, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 0\n",
      "label_mlm: [    0     0     0  4460  4040     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0    30 27619 27751     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0    50  5636     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0   663     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0 18474 27599     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0   172 27665 27699\n",
      "     0     0     0     0     0     0   271 28099  1011 27644     0     0\n",
      "     0   521     0     0     0     0     0     0]\n",
      "\n",
      "{'tokens': ['[CLS]', '▁그러나', '▁주', '[MASK]', '[MASK]', '[MASK]', '▁인', '질', '[MASK]', '▁인', '질', '▁구출', '▁실패를', '▁이유로', '▁1980', '년', '▁대통령', '▁선거에서', '▁공화', '당의', '▁로', '널드', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁', '져', '▁주기도', '▁재', '선에', '▁실패했다', '.', '▁또한', '▁임기', '▁말기에', '▁터', '진', '▁소련의', '▁아프가니스탄', '▁침공', '▁사건으로', '▁인해', '▁1980', '년', '▁하계', '▁올림픽에', '▁반공', '국가', '들의', '▁보이', '콧', '을', '[MASK]', '[MASK]', '[MASK]', '[SEP]', '▁지미', '▁카', '터는', '▁대한민국', '과의', '▁관계', '에서도', '▁중요한', '▁영향을', '▁미', '쳤던', '▁대통령', '▁중', '▁하나다', '.', '▁인권', '▁문제와', '▁주한미', '군', '▁다룰', '▁문제로', '▁한때', '▁한미', '▁관계가', '▁불편', '하기도', '▁했다', '.', '▁1978', '년', '▁대한민국에', '[MASK]', '▁북한의', '▁위협', '에', '▁대비해', '▁한미', '연합', '사를', '[MASK]', '[MASK]', '[MASK]', '▁1982', '년까지', '▁3', '단', '계에', '▁걸쳐', '▁주한미', '군을', '▁철수', '하기로', '▁했다', '.', '[MASK]', '▁주한미', '군', '사령', '부와', '▁정보', '기관', '·', '의', '회의', '▁반', '대에', '▁부딪', '혀', '▁주한미', '군은', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [3, 4, 5, 8, 22, 23, 24, 25, 28, 53, 54, 55, 76, 88, 96, 97, 98, 111], 'mask_label': ['▁이란', '▁미국', '▁대사관', '▁사건에서', '▁레이', '건', '▁후보', '에게', '▁결국', '▁내세', '웠다', '.', '▁철수', '▁대한', '▁창설', '하면서', ',', '▁그러나']}\n",
      "enc_token: [5, 330, 37, 6, 6, 6, 42, 27892, 6, 42, 27892, 11560, 22684, 1827, 1640, 27625, 663, 5249, 4460, 1547, 194, 8631, 6, 6, 6, 6, 27596, 27944, 25601, 174, 2087, 9510, 27599, 276, 11034, 12145, 870, 27713, 5569, 7676, 3232, 6322, 751, 1640, 27625, 2219, 5825, 13948, 4398, 247, 3052, 28805, 27607, 6, 6, 6, 4, 16415, 207, 4612, 410, 786, 704, 643, 1165, 1063, 55, 23859, 663, 35, 15550, 27599, 5636, 14964, 24438, 27722, 21040, 4875, 3590, 12259, 4857, 12019, 863, 345, 27599, 3331, 27625, 11525, 6, 9305, 3038, 27600, 25557, 12259, 2569, 451, 6, 6, 6, 2760, 673, 49, 27737, 1949, 1633, 24438, 1262, 5337, 2390, 345, 27599, 6, 24438, 27722, 3069, 2576, 1071, 1468, 27873, 27601, 511, 141, 867, 11574, 28178, 24438, 941, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 1\n",
      "label_mlm: [    0     0     0  3290   243 18590     0     0 23937     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0  1169 27803\n",
      "   958   113     0     0   875     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0  5890  1853 27599     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0  5337     0     0     0     0     0     0     0\n",
      "     0     0     0     0    92     0     0     0     0     0     0     0\n",
      "  3574   421 27604     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0   330     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n",
      "\n",
      "{'tokens': ['[CLS]', '▁1979', '년', '▁~', '▁1980', '년', '[MASK]', '▁정치적', '▁격', '변', '기', '▁당시의', '▁대통령', '이었던', '▁그는', '▁이에', '[MASK]', '▁애매', '한', '▁태도를', '▁보였고', ',', '▁이는', '▁후에', '▁대한민국', '▁내에서', '▁고조', '되는', '▁반', '미', '▁운동의', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁10', '월', '[MASK]', '[MASK]', '[MASK]', '▁박정희', '▁대통령이', '▁김재', '규', '▁중앙정보', '부', '장에', '[MASK]', '▁살해', '된', '▁가문', '▁대해', '▁그는', '▁이', '▁사건으로', '▁큰', '▁충격을', '▁받았으며', ',', '▁사이', '러스', '▁밴', '스', '▁국무', '장', '관을', '▁조', '[SEP]', '▁퇴임', '▁이후', '▁민간', '▁자원을', '▁적극', '▁활용한', '▁비영리', '▁기구', '인', '▁카터', '▁재', '단을', '[MASK]', '▁뒤', '▁민주주의', '[MASK]', '[MASK]', '▁위해', '▁제', '▁3', '세계의', '▁선거', '▁감시', '▁활동', '▁및', '▁기니', '▁안톤', '▁맡았다', '숫', '▁의한', '▁드라', '쿤', '쿠르', '스', '▁질병', '▁방', '재를', '▁위해', '▁힘썼다', '.', '▁미국의', '▁빈곤', '층', '▁지원', '▁활동', ',', '▁사랑의', '▁집', '짓', '기', '▁운동', ',', '▁국제', '▁분쟁', '▁중재', '▁등의', '▁활동도', '▁했다', '.', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [6, 16, 31, 32, 33, 34, 37, 38, 39, 47, 50, 80, 83, 84, 94, 95, 96, 114], 'mask_label': ['▁대한민국의', '▁대해', '▁한', '▁원인이', '▁됐다', '.', '▁26', '일', ',', '▁의해', '▁것에', '▁설립한', '▁실현', '을', '▁벌', '레', '에', '▁사랑의']}\n",
      "enc_token: [5, 2995, 27625, 203, 1640, 27625, 6, 2843, 1032, 27889, 27614, 3195, 663, 1277, 202, 695, 6, 26003, 27612, 11162, 19177, 27604, 594, 1140, 410, 3428, 8964, 267, 141, 27686, 8711, 6, 6, 6, 6, 131, 27662, 6, 6, 6, 5298, 4864, 9918, 27958, 18525, 27638, 1312, 6, 2591, 27711, 1980, 433, 202, 8, 6322, 459, 10688, 5325, 27604, 328, 2086, 1228, 27626, 4444, 27651, 1657, 53, 4, 13826, 165, 3174, 17304, 2929, 20639, 16068, 6673, 27628, 25250, 174, 1574, 6, 339, 9889, 6, 6, 231, 30, 49, 21655, 822, 7049, 375, 228, 18137, 25480, 1896, 28535, 1332, 17378, 28956, 16453, 27626, 5225, 95, 4445, 231, 19607, 27599, 679, 14197, 28083, 770, 375, 27604, 14003, 313, 28333, 27614, 887, 27604, 605, 4476, 13267, 507, 27328, 345, 27599, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 1\n",
      "label_mlm: [    0     0     0     0     0     0   447     0     0     0     0     0\n",
      "     0     0     0     0   433     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0    34  6418  3842 27599     0\n",
      "     0   981 27629 27604     0     0     0     0     0     0     0   355\n",
      "     0     0  2057     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0  7301     0     0  5031\n",
      " 27607     0     0     0     0     0     0     0     0     0   813 27740\n",
      " 27600     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0 14003     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n",
      "\n",
      "{'tokens': ['[CLS]', '▁1978', '년에', '▁채', '결', '된', '▁캠프', '데이', '비', '드', '▁협', '정의', '▁이', '행이', '▁지지', '부', '진', '▁하자', '▁중동', '▁분쟁', '▁분', '제를', '[MASK]', '▁위해', '[MASK]', '[MASK]', '▁퇴임', '▁후', '▁직접', '▁이스라엘', '과', '▁팔', '레인', '스타', '인의', '▁오슬로', '▁협정을', '▁이끌어', '▁내는', '▁데', '도', '▁성공했다', '.', '[SEP]', '▁카', '터는', '▁카터', '▁행정부', '▁이후', '▁미국이', '▁북', '핵', '▁위기', ',', '[MASK]', '▁전쟁', ',', '▁이라크', '▁전쟁과', '▁같이', '[MASK]', '▁군사적', '▁행동을', '▁최후', '로', '▁선택하는', '▁전통적', '▁사고를', '[MASK]', '▁군사적', '▁행동을', '▁립', '멎', '▁행위에', '▁대해', '▁깊은', '[MASK]', '[MASK]', '▁명령에', '▁하며', '▁미국의', '▁군사적', '▁활동에', '▁강한', '▁반대', '▁입장을', '[MASK]', '▁있다', '.', '[MASK]', '▁국제', '▁분쟁', '▁조정을', '[MASK]', '▁북한의', '▁김일성', ',', '[MASK]', '[MASK]', '▁세', '드', '라스', '▁장군', ',', '▁팔', '레인', '스타', '인의', '▁하', '마스', ',', '▁보스', '니아의', '▁세르비아', '계', '▁정권', '▁같이', '▁미국', '▁정부에', '▁대해', '▁협상을', '▁거부', '하면서', '▁사태', '의', '▁위기를', '▁초래', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [20, 21, 22, 24, 25, 54, 60, 68, 71, 72, 76, 77, 78, 86, 89, 93, 97, 98], 'mask_label': ['▁분', '제를', '▁해결하기', '▁1993', '년', '▁코소보', '▁미국이', '▁버리고', '▁선행', '하는', '▁유', '감을', '▁표시', '▁보이고', '▁특히', '▁위해', '▁아이', '티의']}\n",
      "enc_token: [5, 3331, 169, 481, 27783, 27711, 10251, 4282, 27694, 27681, 617, 2028, 8, 7071, 1565, 27638, 27713, 3121, 8021, 4476, 147, 1104, 6, 231, 6, 6, 13826, 81, 1069, 3426, 27644, 961, 5346, 936, 692, 25213, 15990, 7027, 7573, 189, 27627, 6608, 27599, 4, 207, 4612, 25250, 21862, 165, 8424, 251, 28166, 10622, 27604, 6, 506, 27604, 6157, 17305, 733, 6, 9641, 5616, 12241, 27603, 26028, 15644, 13098, 6, 9641, 5616, 22507, 31789, 19690, 433, 4508, 6, 6, 17109, 1368, 679, 9641, 8253, 2632, 1216, 5168, 6, 28, 27599, 6, 605, 4476, 23144, 6, 9305, 11444, 27604, 6, 6, 74, 27681, 1951, 4379, 27604, 961, 5346, 936, 692, 27, 3678, 27604, 6076, 4174, 4543, 27704, 5752, 733, 243, 6633, 433, 12149, 2324, 421, 4597, 27601, 11239, 8200, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 0\n",
      "label_mlm: [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0   147  1104 13425     0\n",
      "  2062 27625     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0 26202     0     0     0     0     0\n",
      "  8424     0     0     0     0     0     0     0  8275     0     0 16374\n",
      "    38     0     0     0    46  2196  2466     0     0     0     0     0\n",
      "     0     0  7010     0     0   698     0     0     0   231     0     0\n",
      "     0   520 10694     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n",
      "\n",
      "{'tokens': ['[CLS]', '▁1993', '년', '▁1', '차', '▁북', '핵', '▁위기', '▁당시', '▁북한에', '▁대한', '▁거절하였다', '▁군사적', '▁행동이', '▁임', '박', '했으나', ',', '▁미국', '친다', '[MASK]', '[MASK]', '▁처음으로', '▁북한', '을', '▁방문', '하고', '▁미국과', '[MASK]', '▁양국의', '▁중재', '에', '▁큰', '▁기여를', '▁해', '▁위기를', '▁해결', '했다는', '▁평가를', '▁받았다', '.', '▁또한', '▁이', '▁때', '▁김영삼', '▁대통령과', '▁김일성', '▁주', '석의', '▁만남', '을', '▁주선', '했다', '.', '▁하지만', '▁그로부터', '▁수', '주일', '▁후', '▁김일', '성이', '[MASK]', '▁사망하여', '▁김일', '[SEP]', '▁미국의', '▁관', '타나', '모', '▁수용', '소', '▁문제', ',', '▁세계의', '▁인권', '문제', '에서도', '▁관심이', '▁깊', '어', '▁유엔', '에', '▁유엔', '인권', '고등', '판', '무', '관의', '[MASK]', '▁시행', '하도록', '▁노력', '하여', '[MASK]', '[MASK]', '▁인권', '▁유', '린', '에', '[MASK]', '▁제', '약을', '▁하고', ',', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁데', '[MASK]', '[MASK]', '▁독재', '자들', '▁같은', '▁인권', '유', '린', '범죄', '자를', '▁재판', '소로', '▁회', '부', '하여', '[MASK]', '▁처벌을', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [11, 19, 20, 21, 28, 61, 88, 93, 94, 99, 104, 105, 106, 107, 108, 110, 111, 125], 'mask_label': ['▁미국의', '▁전직', '▁대통령', '으로는', '▁북', '▁갑자기', '▁제도를', '▁독재', '자들의', '▁대해', '▁국제', '형사', '재판', '소를', '▁만드는', '▁기여', '하여', '▁국제적인']}\n",
      "enc_token: [5, 2062, 27625, 7, 27751, 251, 28166, 10622, 316, 25086, 92, 18870, 9641, 18507, 273, 27914, 1003, 27604, 243, 6270, 6, 6, 1307, 1876, 27607, 2017, 48, 5672, 6, 25764, 13267, 27600, 459, 13856, 87, 11239, 2317, 2351, 4549, 772, 27599, 276, 8, 84, 9133, 13799, 11444, 37, 5361, 11842, 27607, 25754, 31, 27599, 589, 14313, 19, 10106, 81, 4636, 684, 6, 26809, 4636, 4, 679, 88, 8011, 27716, 2237, 27688, 550, 27604, 5467, 5636, 5515, 643, 8181, 1910, 27633, 3708, 27600, 3708, 12972, 5059, 27841, 27725, 2429, 6, 2404, 1816, 3375, 54, 6, 6, 5636, 46, 27870, 27600, 6, 30, 3297, 644, 27604, 6, 6, 6, 6, 6, 189, 6, 6, 7559, 3989, 226, 5636, 27690, 27870, 8822, 690, 2474, 2661, 270, 27638, 54, 6, 19956, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 1\n",
      "label_mlm: [    0     0     0     0     0     0     0     0     0     0     0   679\n",
      "     0     0     0     0     0     0     0  5605   663  1030     0     0\n",
      "     0     0     0     0   251     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0  5908     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0  6520     0     0     0     0  7559  2653     0\n",
      "     0     0     0   433     0     0     0     0   605 17905  4731  1358\n",
      "  3002     0  2187    54     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0 12835     0     0]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 라인 단위로 처리\n",
    "with open(pretrain_json_path, \"r\") as f:\n",
    "    for i, line in enumerate(tqdm(f, total=total)):\n",
    "        if 5 < i:  # 테스트를 위해서 5개만 확인\n",
    "            break\n",
    "        data = json.loads(line)\n",
    "        # encoder token\n",
    "        enc_token = [vocab.piece_to_id(p) for p in data[\"tokens\"]]\n",
    "        enc_token += [0] * (n_seq - len(enc_token))\n",
    "        # segment\n",
    "        segment = data[\"segment\"]\n",
    "        segment += [0] * (n_seq - len(segment))\n",
    "        # nsp label\n",
    "        label_nsp = data[\"is_next\"]\n",
    "        # mlm label\n",
    "        mask_idx = np.array(data[\"mask_idx\"], dtype=np.int)\n",
    "        mask_label = np.array([vocab.piece_to_id(p) for p in data[\"mask_label\"]], dtype=np.int)\n",
    "        label_mlm = np.full(n_seq, dtype=np.int, fill_value=0)\n",
    "        label_mlm[mask_idx] = mask_label\n",
    "\n",
    "        print(data)\n",
    "        print(\"enc_token:\", enc_token)\n",
    "        print(\"segment:\", segment)\n",
    "        print(\"label_nsp:\", label_nsp)\n",
    "        print(\"label_mlm:\", label_mlm)\n",
    "        print()\n",
    "\n",
    "        assert len(enc_token) == len(segment) == len(label_mlm) == n_seq\n",
    "\n",
    "        enc_tokens[i] = enc_token\n",
    "        segments[i] = segment\n",
    "        labels_nsp[i] = label_nsp\n",
    "        labels_mlm[i] = label_mlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "Wltw84G4hFKA"
   },
   "outputs": [],
   "source": [
    "def load_pre_train_data(vocab, filename, n_seq, count=None):\n",
    "    \"\"\"\n",
    "    학습에 필요한 데이터를 로드\n",
    "    :param vocab: vocab\n",
    "    :param filename: 전처리된 json 파일\n",
    "    :param n_seq: 시퀀스 길이 (number of sequence)\n",
    "    :param count: 데이터 수 제한 (None이면 전체)\n",
    "    :return enc_tokens: encoder inputs\n",
    "    :return segments: segment inputs\n",
    "    :return labels_nsp: nsp labels\n",
    "    :return labels_mlm: mlm labels\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            total += 1\n",
    "            # 데이터 수 제한\n",
    "            if count is not None and count <= total:\n",
    "                break\n",
    "\n",
    "    # np.memmap을 사용하면 메모리를 적은 메모리에서도 대용량 데이터 처리가 가능 함\n",
    "    enc_tokens = np.memmap(filename='enc_tokens.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "    segments = np.memmap(filename='segments.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "    labels_nsp = np.memmap(filename='labels_nsp.memmap', mode='w+', dtype=np.int32, shape=(total,))\n",
    "    labels_mlm = np.memmap(filename='labels_mlm.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "\n",
    "    with open(filename, \"r\") as f:\n",
    "        for i, line in enumerate(tqdm(f, total=total)):\n",
    "            if total <= i:\n",
    "                print(\"data load early stop\", total, i)\n",
    "                break\n",
    "            data = json.loads(line)\n",
    "            # encoder token\n",
    "            enc_token = [vocab.piece_to_id(p) for p in data[\"tokens\"]]\n",
    "            enc_token += [0] * (n_seq - len(enc_token))\n",
    "            # segment\n",
    "            segment = data[\"segment\"]\n",
    "            segment += [0] * (n_seq - len(segment))\n",
    "            # nsp label\n",
    "            label_nsp = data[\"is_next\"]\n",
    "            # mlm label\n",
    "            mask_idx = np.array(data[\"mask_idx\"], dtype=np.int)\n",
    "            mask_label = np.array([vocab.piece_to_id(p) for p in data[\"mask_label\"]], dtype=np.int)\n",
    "            label_mlm = np.full(n_seq, dtype=np.int, fill_value=0)\n",
    "            label_mlm[mask_idx] = mask_label\n",
    "\n",
    "            assert len(enc_token) == len(segment) == len(label_mlm) == n_seq\n",
    "\n",
    "            enc_tokens[i] = enc_token\n",
    "            segments[i] = segment\n",
    "            labels_nsp[i] = label_nsp\n",
    "            labels_mlm[i] = label_mlm\n",
    "\n",
    "    return (enc_tokens, segments), (labels_nsp, labels_mlm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "Gz8H8T40hFIN"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/128000 [00:00<?, ?it/s]/tmp/ipykernel_31/3092669743.py:42: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  mask_idx = np.array(data[\"mask_idx\"], dtype=np.int)\n",
      "/tmp/ipykernel_31/3092669743.py:43: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  mask_label = np.array([vocab.piece_to_id(p) for p in data[\"mask_label\"]], dtype=np.int)\n",
      "/tmp/ipykernel_31/3092669743.py:44: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  label_mlm = np.full(n_seq, dtype=np.int, fill_value=0)\n",
      "100%|██████████| 128000/128000 [00:25<00:00, 4933.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data load early stop 128000 128000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 128000건만 메모리에 로딩\n",
    "pre_train_inputs, pre_train_labels = load_pre_train_data(vocab, pretrain_json_path, 128, count=128000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "3cxqPSS3hFGX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(memmap([    5, 16415, 25250,  3324,  1042,   103, 27610, 27686, 27718,\n",
       "             6,     6,     6,  5708, 27625, 29622, 31759,     7, 27629,\n",
       "           203,   241, 27602,  4867,   788,     6,  5898,   796,     6,\n",
       "          1647,  4630, 27625,   203,  3008, 27625, 27616,    16, 27599,\n",
       "         16415,   207,  4612,     6,     6,   630, 27714,     6,   429,\n",
       "          5346, 27626, 14406,  1605, 27599,     6, 14146, 15991,  8637,\n",
       "         27599,    13,    81, 25987,  2247, 15033, 27873, 14475, 27813,\n",
       "         27873,     4,  3715, 27625,  5551,    37, 11234,     6,  5249,\n",
       "          9858,  3294,    13,  1636,  2386,  2163, 27596, 27671,   969,\n",
       "          8047,   173,   607,     6,     6,     6,  3926, 27625,  5551,\n",
       "            37, 18995,  8198,  9858,  1447,  1921, 27625,  5551,    37,\n",
       "            18,   451,  4267, 27599, 25536,  6436,    25,  5551, 27646,\n",
       "         18205,   928,   157, 27821,    61, 27773,   530, 27604,  3372,\n",
       "           523,  3409,   673,     6,    18,   982, 13264, 27599,  5551,\n",
       "          5053,     4], dtype=int32),\n",
       " memmap([    5,     6,     6,     6,    19,  4791,  3220, 27635,  7462,\n",
       "         13054,    98,  5947, 27616, 27602, 13069, 26245,     6,  8801,\n",
       "         27600,     6,  3220, 27635, 21271,     8, 13676,    68,  7336,\n",
       "             6,     6,     6,  5078,     6,     6,   342,  4812, 27625,\n",
       "           294, 14456,    89, 24930,  2540, 27600,   488,  4871,  2524,\n",
       "         13358,   171, 27599,   330, 27604,     6, 15170,  4842, 27682,\n",
       "         27625,  2131, 14135,  9943,   761, 28254,   658,   171, 27599,\n",
       "            13,     4,   371, 27604,    29, 28018, 27793,  1326, 27787,\n",
       "            66,   412,     6,    28, 27599,  1326,  5884, 11497,  2573,\n",
       "            66,  1599, 27653,   639,  3883,   353, 27599,     6,     6,\n",
       "          1326,  5884,    19,   402,  9141,     6,     6,     6,  2042,\n",
       "          2742,    31, 27599, 11364, 16343,  1911, 27604,    68, 27650,\n",
       "         27617,  3065, 28116, 27601, 10603, 27616,     9, 25545,  1599,\n",
       "         27653,   639,   254,   238,   787,  2654,   784, 27604,  1925,\n",
       "           259,     4], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32),\n",
       " 1,\n",
       " 0,\n",
       " memmap([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "         25250,  7504,   416,     0,     0,   131, 27662,     0,     0,\n",
       "             0,     0,     0,     0,     0,   243,     0,     0,   663,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,  5551, 27646,     0,     0,  4269,     0,\n",
       "             0,     0,     0,     0,     0,  5551,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,  2378,     0,\n",
       "             0,     0,     0, 20590,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,  2387,   317, 27604,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,  4864,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,  5551,     0,     0,     0,     0,     0,\n",
       "             0,     0], dtype=int32),\n",
       " memmap([    0,  3220, 27635,  4937,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,  7203,     0,\n",
       "             0, 19200,     0,     0,     0,     0,     0,     0,     0,\n",
       "         16230,  2191, 27599,     0,    81, 27604,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0, 14456,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0, 22289,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0, 17506,  5263,\n",
       "             0,     0,     0,     0,   231, 11497,  9933, 27607,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0], dtype=int32))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 처음과 마지막 확인\n",
    "pre_train_inputs[0][0], pre_train_inputs[0][-1], pre_train_inputs[1][0], pre_train_inputs[1][-1], pre_train_labels[0][0], pre_train_labels[0][-1], pre_train_labels[1][0], pre_train_labels[1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "rgadW_UFhFEk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def get_pad_mask(tokens, i_pad=0):\n",
    "    \"\"\"\n",
    "    pad mask 계산하는 함수\n",
    "    :param tokens: tokens (bs, n_seq)\n",
    "    :param i_pad: id of pad\n",
    "    :return mask: pad mask (pad: 1, other: 0)\n",
    "    \"\"\"\n",
    "    mask = tf.cast(tf.math.equal(tokens, i_pad), tf.float32)\n",
    "    mask = tf.expand_dims(mask, axis=1)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def get_ahead_mask(tokens, i_pad=0):\n",
    "    \"\"\"\n",
    "    ahead mask 계산하는 함수\n",
    "    :param tokens: tokens (bs, n_seq)\n",
    "    :param i_pad: id of pad\n",
    "    :return mask: ahead and pad mask (ahead or pad: 1, other: 0)\n",
    "    \"\"\"\n",
    "    n_seq = tf.shape(tokens)[1]\n",
    "    ahead_mask = 1 - tf.linalg.band_part(tf.ones((n_seq, n_seq)), -1, 0)\n",
    "    ahead_mask = tf.expand_dims(ahead_mask, axis=0)\n",
    "    pad_mask = get_pad_mask(tokens, i_pad)\n",
    "    mask = tf.maximum(ahead_mask, pad_mask)\n",
    "    return mask\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "rqBX4D_zhFCd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "@tf.function(experimental_relax_shapes=True)\n",
    "def gelu(x):\n",
    "    \"\"\"\n",
    "    gelu activation 함수\n",
    "    :param x: 입력 값\n",
    "    :return: gelu activation result\n",
    "    \"\"\"\n",
    "    return 0.5*x*(1+tf.tanh(np.sqrt(2/np.pi)*(x+0.044715*tf.pow(x, 3))))\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "oV8srxVjhFAe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def kernel_initializer(stddev=0.02):\n",
    "    \"\"\"\n",
    "    parameter initializer 생성\n",
    "    :param stddev: 생성할 랜덤 변수의 표준편차\n",
    "    \"\"\"\n",
    "    return tf.keras.initializers.TruncatedNormal(stddev=stddev)\n",
    "\n",
    "\n",
    "def bias_initializer():\n",
    "    \"\"\"\n",
    "    bias initializer 생성\n",
    "    \"\"\"\n",
    "    return tf.zeros_initializer\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "5tBeSh_xhE-e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class Config(dict):\n",
    "    \"\"\"\n",
    "    json을 config 형태로 사용하기 위한 Class\n",
    "    :param dict: config dictionary\n",
    "    \"\"\"\n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, file):\n",
    "        \"\"\"\n",
    "        file에서 Config를 생성 함\n",
    "        :param file: filename\n",
    "        \"\"\"\n",
    "        with open(file, 'r') as f:\n",
    "            config = json.loads(f.read())\n",
    "            return Config(config)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "hq922rCYhE8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class SharedEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Weighed Shaed Embedding Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"weight_shared_embedding\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.n_vocab = config.n_vocab\n",
    "        self.d_model = config.d_model\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "        shared weight 생성\n",
    "        :param input_shape: Tensor Shape (not used)\n",
    "        \"\"\"\n",
    "        with tf.name_scope(\"shared_embedding_weight\"):\n",
    "            self.shared_weights = self.add_weight(\n",
    "                \"weights\",\n",
    "                shape=[self.n_vocab, self.d_model],\n",
    "                initializer=kernel_initializer()\n",
    "            )\n",
    "\n",
    "    def call(self, inputs, mode=\"embedding\"):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: 입력\n",
    "        :param mode: 실행 모드\n",
    "        :return: embedding or linear 실행 결과\n",
    "        \"\"\"\n",
    "        # mode가 embedding일 경우 embedding lookup 실행\n",
    "        if mode == \"embedding\":\n",
    "            return self._embedding(inputs)\n",
    "        # mode가 linear일 경우 linear 실행\n",
    "        elif mode == \"linear\":\n",
    "            return self._linear(inputs)\n",
    "        # mode가 기타일 경우 오류 발생\n",
    "        else:\n",
    "            raise ValueError(f\"mode {mode} is not valid.\")\n",
    "\n",
    "    def _embedding(self, inputs):\n",
    "        \"\"\"\n",
    "        embedding lookup\n",
    "        :param inputs: 입력\n",
    "        \"\"\"\n",
    "        embed = tf.gather(self.shared_weights, tf.cast(inputs, tf.int32))\n",
    "        return embed\n",
    "\n",
    "    def _linear(self, inputs):  # (bs, n_seq, d_model)\n",
    "        \"\"\"\n",
    "        linear 실행\n",
    "        :param inputs: 입력\n",
    "        \"\"\"\n",
    "        n_batch = tf.shape(inputs)[0]\n",
    "        n_seq = tf.shape(inputs)[1]\n",
    "        inputs = tf.reshape(inputs, [-1, self.d_model])  # (bs * n_seq, d_model)\n",
    "        outputs = tf.matmul(inputs, self.shared_weights, transpose_b=True)\n",
    "        outputs = tf.reshape(outputs, [n_batch, n_seq, self.n_vocab])  # (bs, n_seq, n_vocab)\n",
    "        return outputs\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "NteDh8kfhE6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class PositionEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Position Embedding Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"position_embedding\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(config.n_seq, config.d_model, embeddings_initializer=kernel_initializer())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: 입력\n",
    "        :return embed: position embedding lookup 결과\n",
    "        \"\"\"\n",
    "        position = tf.cast(tf.math.cumsum(tf.ones_like(inputs), axis=1, exclusive=True), tf.int32)\n",
    "        embed = self.embedding(position)\n",
    "        return embed\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "z2N7LcM7hE4Y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class ScaleDotProductAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Scale Dot Product Attention Class\n",
    "    \"\"\"\n",
    "    def __init__(self, name=\"scale_dot_product_attention\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "    def call(self, Q, K, V, attn_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param Q: Q value\n",
    "        :param K: K value\n",
    "        :param V: V value\n",
    "        :param attn_mask: 실행 모드\n",
    "        :return attn_out: attention 실행 결과\n",
    "        \"\"\"\n",
    "        attn_score = tf.matmul(Q, K, transpose_b=True)\n",
    "        scale = tf.math.sqrt(tf.cast(tf.shape(K)[-1], tf.float32))\n",
    "        attn_scale = tf.math.divide(attn_score, scale)\n",
    "        attn_scale -= 1.e9 * attn_mask\n",
    "        attn_prob = tf.nn.softmax(attn_scale, axis=-1)\n",
    "        attn_out = tf.matmul(attn_prob, V)\n",
    "        return attn_out\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "MNXGz3N5hE2K"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# Q. 주석과 코드를 참조하여 아래 클래스를 완성해주세요.\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Multi Head Attention Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"multi_head_attention\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.d_model = config.d_model\n",
    "        self.n_head = config.n_head\n",
    "        self.d_head = config.d_head\n",
    "\n",
    "        # Q, K, V input dense layer\n",
    "        self.W_Q = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_K = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_V = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        # Scale Dot Product Attention class\n",
    "        self.attention = ScaleDotProductAttention(name=\"self_attention\")\n",
    "        # output dense layer\n",
    "        self.W_O = tf.keras.layers.Dense(config.d_model, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "\n",
    "    def call(self, Q, K, V, attn_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param Q: Q value\n",
    "        :param K: K value\n",
    "        :param V: V value\n",
    "        :param attn_mask: 실행 모드\n",
    "        :return attn_out: attention 실행 결과\n",
    "        \"\"\"\n",
    "        # reshape Q, K, V, attn_mask\n",
    "        batch_size = tf.shape(Q)[0]\n",
    "        Q_m = tf.transpose(tf.reshape(self.W_Q(Q), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, Q_len, d_head)\n",
    "        K_m = tf.transpose(tf.reshape(self.W_K(K), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n",
    "        V_m = tf.transpose(tf.reshape(self.W_V(V), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n",
    "        attn_mask_m = tf.expand_dims(attn_mask, axis=1)\n",
    "        # Scale Dot Product Attention with multi head Q, K, V, attn_mask\n",
    "        attn_out = self.attention(Q_m, K_m, V_m, attn_mask_m)  # (bs, n_head, Q_len, d_head)\n",
    "        # transpose and liner\n",
    "        attn_out_m = tf.transpose(attn_out, perm=[0, 2, 1, 3])  # (bs, Q_len, n_head, d_head)\n",
    "        attn_out = tf.reshape(attn_out_m, [batch_size, -1, config.n_head * config.d_head])  # (bs, Q_len, d_model)\n",
    "        attn_out = self.W_O(attn_out) # (bs, Q_len, d_model)\n",
    "\n",
    "        return attn_out\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "Spgi3TjQhE0D"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class PositionWiseFeedForward(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Position Wise Feed Forward Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"feed_forward\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.W_1 = tf.keras.layers.Dense(config.d_ff, activation=gelu, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_2 = tf.keras.layers.Dense(config.d_model, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: inputs\n",
    "        :return ff_val: feed forward 실행 결과\n",
    "        \"\"\"\n",
    "        ff_val = self.W_2(self.W_1(inputs))\n",
    "        return ff_val\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "zlBkjmtwh6ty"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Encoder Layer Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"encoder_layer\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.self_attention = MultiHeadAttention(config)\n",
    "        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "\n",
    "        self.ffn = PositionWiseFeedForward(config)\n",
    "        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    "\n",
    "    def call(self, enc_embed, self_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param enc_embed: enc_embed 또는 이전 EncoderLayer의 출력\n",
    "        :param self_mask: enc_tokens의 pad mask\n",
    "        :return enc_out: EncoderLayer 실행 결과\n",
    "        \"\"\"\n",
    "        self_attn_val = self.self_attention(enc_embed, enc_embed, enc_embed, self_mask)\n",
    "        norm1_val = self.norm1(enc_embed + self.dropout(self_attn_val))\n",
    "\n",
    "        ffn_val = self.ffn(norm1_val)\n",
    "        enc_out = self.norm2(norm1_val + self.dropout(ffn_val))\n",
    "\n",
    "        return enc_out\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "rUaU3ESuh6rs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class BERT(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    BERT Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"bert\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.i_pad = config.i_pad\n",
    "        self.embedding = SharedEmbedding(config)\n",
    "        self.position = PositionEmbedding(config)\n",
    "        self.segment = tf.keras.layers.Embedding(2, config.d_model, embeddings_initializer=kernel_initializer())\n",
    "        self.norm = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "\n",
    "        self.encoder_layers = [EncoderLayer(config, name=f\"encoder_layer_{i}\") for i in range(config.n_layer)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: (enc_tokens, segments)\n",
    "        :return logits: dec_tokens에 대한 다음 토큰 예측 결과 logits\n",
    "        \"\"\"\n",
    "        enc_tokens, segments = inputs\n",
    "\n",
    "        enc_self_mask = tf.keras.layers.Lambda(get_pad_mask, output_shape=(1, None), name='enc_self_mask')(enc_tokens, self.i_pad)\n",
    "\n",
    "        enc_embed = self.get_embedding(enc_tokens, segments)\n",
    "\n",
    "        enc_out = self.dropout(enc_embed)\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            enc_out = encoder_layer(enc_out, enc_self_mask)\n",
    "\n",
    "        logits_cls = enc_out[:,0]\n",
    "        logits_lm = self.embedding(enc_out, mode=\"linear\")\n",
    "        return logits_cls, logits_lm\n",
    "\n",
    "    def get_embedding(self, tokens, segments):\n",
    "        \"\"\"\n",
    "        token embedding, position embedding lookup\n",
    "        :param tokens: 입력 tokens\n",
    "        :param segments: 입력 segments\n",
    "        :return embed: embedding 결과\n",
    "        \"\"\"\n",
    "        embed = self.embedding(tokens) + self.position(tokens) + self.segment(segments)\n",
    "        embed = self.norm(embed)\n",
    "        return embed\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "XSV0nLmch6p0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# Encoder Layer class 정의\n",
    "class PooledOutput(tf.keras.layers.Layer):\n",
    "    def __init__(self, config, n_output, name=\"pooled_output\"):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.dense1 = tf.keras.layers.Dense(config.d_model, activation=tf.nn.tanh, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.dense2 = tf.keras.layers.Dense(n_output, use_bias=False, activation=tf.nn.softmax, name=\"nsp\", kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        outputs = self.dense1(inputs)\n",
    "        outputs = self.dense2(outputs)\n",
    "        return outputs\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "eePESpr7h6n9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def build_model_pre_train(config):\n",
    "    enc_tokens = tf.keras.layers.Input((None,), name=\"enc_tokens\")\n",
    "    segments = tf.keras.layers.Input((None,), name=\"segments\")\n",
    "\n",
    "    bert = BERT(config)\n",
    "    logits_cls, logits_lm = bert((enc_tokens, segments))\n",
    "\n",
    "    logits_cls = PooledOutput(config, 2, name=\"pooled_nsp\")(logits_cls)\n",
    "    outputs_nsp = tf.keras.layers.Softmax(name=\"nsp\")(logits_cls)\n",
    "\n",
    "    outputs_mlm = tf.keras.layers.Softmax(name=\"mlm\")(logits_lm)\n",
    "\n",
    "    model = tf.keras.Model(inputs=(enc_tokens, segments), outputs=(outputs_nsp, outputs_mlm))\n",
    "    return model\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "zsqXHs31h6l1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'d_model': 256,\n",
       " 'n_head': 4,\n",
       " 'd_head': 64,\n",
       " 'dropout': 0.1,\n",
       " 'd_ff': 1024,\n",
       " 'layernorm_epsilon': 0.001,\n",
       " 'n_layer': 3,\n",
       " 'n_seq': 256,\n",
       " 'n_vocab': 32007,\n",
       " 'i_pad': 0}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Config({\"d_model\": 256, \"n_head\": 4, \"d_head\": 64, \"dropout\": 0.1, \"d_ff\": 1024, \"layernorm_epsilon\": 0.001, \"n_layer\": 3, \"n_seq\": 256, \"n_vocab\": 0, \"i_pad\": 0})\n",
    "config.n_vocab = len(vocab)\n",
    "config.i_pad = vocab.pad_id()\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "AwInAMuTh6jv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "2/2 [==============================] - 3s 15ms/step - loss: 11.2227 - nsp_loss: 0.7743 - mlm_loss: 10.4484 - nsp_acc: 0.5000 - mlm_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 10.0172 - nsp_loss: 0.6251 - mlm_loss: 9.3921 - nsp_acc: 0.8000 - mlm_acc: 0.0200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7b3956862460>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_seq = 10\n",
    "\n",
    "# make test inputs\n",
    "enc_tokens = np.random.randint(0, len(vocab), (10, n_seq))\n",
    "segments = np.random.randint(0, 2, (10, n_seq))\n",
    "labels_nsp = np.random.randint(0, 2, (10,))\n",
    "labels_mlm = np.random.randint(0, len(vocab), (10, n_seq))\n",
    "\n",
    "test_model = build_model_pre_train(config)\n",
    "test_model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy, optimizer=tf.keras.optimizers.Adam(), metrics=[\"acc\"])\n",
    "\n",
    "# test model fit\n",
    "test_model.fit((enc_tokens, segments), (labels_nsp, labels_mlm), epochs=2, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "HOL8Lr7CpRM4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def lm_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    loss 계산 함수\n",
    "    :param y_true: 정답 (bs, n_seq)\n",
    "    :param y_pred: 예측 값 (bs, n_seq, n_vocab)\n",
    "    \"\"\"\n",
    "    # loss 계산\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)(y_true, y_pred)\n",
    "    # pad(0) 인 부분 mask\n",
    "    mask = tf.cast(tf.math.not_equal(y_true, 0), dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    return loss * 20  # mlm을 더 잘 학습하도록 20배 증가 시킴\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "a1hJVfnjiQ8-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def lm_acc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    acc 계산 함수\n",
    "    :param y_true: 정답 (bs, n_seq)\n",
    "    :param y_pred: 예측 값 (bs, n_seq, n_vocab)\n",
    "    \"\"\"\n",
    "    # 정답 여부 확인\n",
    "    y_pred_class = tf.cast(K.argmax(y_pred, axis=-1), tf.float32)\n",
    "    matches = tf.cast(K.equal(y_true, y_pred_class), tf.float32)\n",
    "    # pad(0) 인 부분 mask\n",
    "    mask = tf.cast(tf.math.not_equal(y_true, 0), dtype=matches.dtype)\n",
    "    matches *= mask\n",
    "    # 정확도 계산\n",
    "    accuracy = K.sum(matches) / K.maximum(K.sum(mask), 1)\n",
    "    return accuracy\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "x534DkT3iQ6_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class CosineSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"\n",
    "    CosineSchedule Class\n",
    "    \"\"\"\n",
    "    def __init__(self, train_steps=4000, warmup_steps=2000, max_lr=2.5e-4):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param train_steps: 학습 step 총 합\n",
    "        :param warmup_steps: warmup steps\n",
    "        :param max_lr: 최대 learning rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        assert 0 < warmup_steps < train_steps\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.train_steps = train_steps\n",
    "        self.max_lr = max_lr\n",
    "\n",
    "    def __call__(self, step_num):\n",
    "        \"\"\"\n",
    "        learning rate 계산\n",
    "        :param step_num: 현재 step number\n",
    "        :retrun: 계산된 learning rate\n",
    "        \"\"\"\n",
    "        state = tf.cast(step_num <= self.warmup_steps, tf.float32)\n",
    "        lr1 = tf.cast(step_num, tf.float32) / self.warmup_steps\n",
    "        progress = tf.cast(step_num - self.warmup_steps, tf.float32) / max(1, self.train_steps - self.warmup_steps)\n",
    "        lr2 = 0.5 * (1.0 + tf.math.cos(math.pi * progress))\n",
    "        return (state * lr1 + (1 - state) * lr2) * self.max_lr\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "bZ6R_hXMiQ3O"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# compute lr\n",
    "test_schedule = CosineSchedule(train_steps=4000, warmup_steps=500)\n",
    "lrs = []\n",
    "for step_num in range(4000):\n",
    "    lrs.append(test_schedule(float(step_num)).numpy())\n",
    "\n",
    "# draw\n",
    "plt.plot(lrs, 'r-', label='learning_rate')\n",
    "plt.xlabel('Step')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "HrFDWfhkiQ1Q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "enc_tokens (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segments (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert (BERT)                     ((None, 256), (None, 10629632    enc_tokens[0][0]                 \n",
      "                                                                 segments[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pooled_nsp (PooledOutput)       (None, 2)            66304       bert[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "nsp (Softmax)                   (None, 2)            0           pooled_nsp[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "mlm (Softmax)                   (None, None, 32007)  0           bert[0][1]                       \n",
      "==================================================================================================\n",
      "Total params: 10,695,936\n",
      "Trainable params: 10,695,936\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 생성\n",
    "pre_train_model = build_model_pre_train(config)\n",
    "pre_train_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "71Si9vvDi4jp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_steps: 20000\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "# optimizer\n",
    "train_steps = math.ceil(len(pre_train_inputs[0]) / batch_size) * epochs\n",
    "print(\"train_steps:\", train_steps)\n",
    "learning_rate = CosineSchedule(train_steps=train_steps, warmup_steps=max(100, train_steps // 10))\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "# compile\n",
    "pre_train_model.compile(loss=(tf.keras.losses.sparse_categorical_crossentropy, lm_loss), optimizer=optimizer, metrics={\"nsp\": \"acc\", \"mlm\": lm_acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_weights = tf.keras.callbacks.ModelCheckpoint(f\"{model_dir}/bert_pre_train.hdf5\", monitor=\"mlm_lm_acc\", verbose=1, save_best_only=True, mode=\"max\", save_freq=\"epoch\", save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "0rKvR-4UiQzf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1800/1800 [==============================] - 521s 288ms/step - loss: 22.8563 - nsp_loss: 0.6502 - mlm_loss: 22.2060 - nsp_acc: 0.5909 - mlm_lm_acc: 0.0835 - val_loss: 21.6461 - val_nsp_loss: 0.6332 - val_mlm_loss: 21.0129 - val_nsp_acc: 0.6084 - val_mlm_lm_acc: 0.1041\n",
      "\n",
      "Epoch 00001: mlm_lm_acc improved from -inf to 0.08353, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 2/10\n",
      "1800/1800 [==============================] - 520s 289ms/step - loss: 20.5047 - nsp_loss: 0.6187 - mlm_loss: 19.8860 - nsp_acc: 0.6341 - mlm_lm_acc: 0.1179 - val_loss: 20.3936 - val_nsp_loss: 0.6029 - val_mlm_loss: 19.7908 - val_nsp_acc: 0.6525 - val_mlm_lm_acc: 0.1212\n",
      "\n",
      "Epoch 00002: mlm_lm_acc improved from 0.08353 to 0.11794, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 3/10\n",
      "1800/1800 [==============================] - 520s 289ms/step - loss: 19.2958 - nsp_loss: 0.5900 - mlm_loss: 18.7058 - nsp_acc: 0.6860 - mlm_lm_acc: 0.1344 - val_loss: 19.6680 - val_nsp_loss: 0.6130 - val_mlm_loss: 19.0550 - val_nsp_acc: 0.6493 - val_mlm_lm_acc: 0.1336\n",
      "\n",
      "Epoch 00003: mlm_lm_acc improved from 0.11794 to 0.13436, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 4/10\n",
      "1800/1800 [==============================] - 521s 289ms/step - loss: 18.0889 - nsp_loss: 0.5641 - mlm_loss: 17.5247 - nsp_acc: 0.7235 - mlm_lm_acc: 0.1522 - val_loss: 18.2069 - val_nsp_loss: 0.6219 - val_mlm_loss: 17.5850 - val_nsp_acc: 0.6447 - val_mlm_lm_acc: 0.1602\n",
      "\n",
      "Epoch 00004: mlm_lm_acc improved from 0.13436 to 0.15223, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 5/10\n",
      "1800/1800 [==============================] - 521s 289ms/step - loss: 16.4378 - nsp_loss: 0.5385 - mlm_loss: 15.8993 - nsp_acc: 0.7573 - mlm_lm_acc: 0.1802 - val_loss: 16.9974 - val_nsp_loss: 0.6382 - val_mlm_loss: 16.3591 - val_nsp_acc: 0.6432 - val_mlm_lm_acc: 0.1817\n",
      "\n",
      "Epoch 00005: mlm_lm_acc improved from 0.15223 to 0.18021, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 6/10\n",
      "1800/1800 [==============================] - 521s 290ms/step - loss: 15.3498 - nsp_loss: 0.5070 - mlm_loss: 14.8428 - nsp_acc: 0.7957 - mlm_lm_acc: 0.2017 - val_loss: 16.4026 - val_nsp_loss: 0.6481 - val_mlm_loss: 15.7545 - val_nsp_acc: 0.6409 - val_mlm_lm_acc: 0.1964\n",
      "\n",
      "Epoch 00006: mlm_lm_acc improved from 0.18021 to 0.20165, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 7/10\n",
      "1800/1800 [==============================] - 521s 290ms/step - loss: 14.6563 - nsp_loss: 0.4812 - mlm_loss: 14.1751 - nsp_acc: 0.8243 - mlm_lm_acc: 0.2170 - val_loss: 16.1041 - val_nsp_loss: 0.6613 - val_mlm_loss: 15.4428 - val_nsp_acc: 0.6355 - val_mlm_lm_acc: 0.2034\n",
      "\n",
      "Epoch 00007: mlm_lm_acc improved from 0.20165 to 0.21701, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 8/10\n",
      "1800/1800 [==============================] - 521s 289ms/step - loss: 14.1609 - nsp_loss: 0.4589 - mlm_loss: 13.7020 - nsp_acc: 0.8493 - mlm_lm_acc: 0.2285 - val_loss: 15.9185 - val_nsp_loss: 0.6598 - val_mlm_loss: 15.2587 - val_nsp_acc: 0.6395 - val_mlm_lm_acc: 0.2096\n",
      "\n",
      "Epoch 00008: mlm_lm_acc improved from 0.21701 to 0.22847, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 9/10\n",
      "1800/1800 [==============================] - 521s 290ms/step - loss: 13.8279 - nsp_loss: 0.4434 - mlm_loss: 13.3845 - nsp_acc: 0.8663 - mlm_lm_acc: 0.2365 - val_loss: 15.8319 - val_nsp_loss: 0.6602 - val_mlm_loss: 15.1717 - val_nsp_acc: 0.6440 - val_mlm_lm_acc: 0.2116\n",
      "\n",
      "Epoch 00009: mlm_lm_acc improved from 0.22847 to 0.23653, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 10/10\n",
      "1800/1800 [==============================] - 521s 290ms/step - loss: 13.6260 - nsp_loss: 0.4334 - mlm_loss: 13.1925 - nsp_acc: 0.8772 - mlm_lm_acc: 0.2413 - val_loss: 15.8007 - val_nsp_loss: 0.6632 - val_mlm_loss: 15.1375 - val_nsp_acc: 0.6405 - val_mlm_lm_acc: 0.2129\n",
      "\n",
      "Epoch 00010: mlm_lm_acc improved from 0.23653 to 0.24129, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "history = pre_train_model.fit(\n",
    "    pre_train_inputs,\n",
    "    pre_train_labels,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=[save_weights],\n",
    "    validation_split=0.1  # 검증 데이터 비율, 필요에 따라 조정하세요\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "id": "3S-ZZ9t_iQxf"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# training result\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['nsp_loss'], 'b-', label='nsp_loss')\n",
    "plt.plot(history.history['mlm_loss'], 'r--', label='mlm_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['nsp_acc'], 'g-', label='nsp_acc')\n",
    "plt.plot(history.history['mlm_lm_acc'], 'k--', label='mlm_acc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LP5YvxRQiQvn"
   },
   "source": [
    "# 회고"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZryAG0eDiQth"
   },
   "source": [
    "- **10번째 에폭 결과**\n",
    "\n",
    "```\n",
    "Epoch 10/10\n",
    "1800/1800 [==============================] - 521s 290ms/step - loss: 13.6260 - nsp_loss: 0.4334 - mlm_loss: 13.1925 - nsp_acc: 0.8772 - mlm_lm_acc: 0.2413 - val_loss: 15.8007 - val_nsp_loss: 0.6632 - val_mlm_loss: 15.1375 - val_nsp_acc: 0.6405 - val_mlm_lm_acc: 0.2129\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QUBtf-KCiQro"
   },
   "source": [
    "1. 손실(Loss) 지표:\n",
    "\n",
    "    - 전체 손실(loss): 13.6260\n",
    "\n",
    "    - NSP(Next Sentence Prediction) 손실: 0.4334\n",
    "\n",
    "    - MLM(Masked Language Model) 손실: 13.1925\n",
    "\n",
    "훈련 세트에서의 전체 손실은 NSP와 MLM 손실의 합으로 계산됩니다.\n",
    "\n",
    "2. 정확도(Accuracy) 지표:\n",
    "\n",
    "    - NSP 정확도: 0.8772 (87.72%)\n",
    "\n",
    "    - MLM 정확도: 0.2413 (24.13%)\n",
    "\n",
    "NSP 정확도가 높은 것은 모델이 문장 간의 관계를 잘 예측하고 있음을 의미합니다.\n",
    "\n",
    "3. 검증(Validation) 지표:\n",
    "\n",
    "    - 검증 손실: 15.8007\n",
    "\n",
    "    - 검증 NSP 손실: 0.6632\n",
    "\n",
    "    - 검증 MLM 손실: 15.1375\n",
    "\n",
    "    - 검증 NSP 정확도: 0.6405 (64.05%)\n",
    "\n",
    "    - 검증 MLM 정확도: 0.2129 (21.29%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0oMYgQ2yiQpa"
   },
   "source": [
    "### loss는 전반적으로 줄고 accuracy는 상승하는 결과를 보입니다! 나름 학습이 잘 된 것 같습니다!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
