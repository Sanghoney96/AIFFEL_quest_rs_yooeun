{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "557418c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import sentencepiece as spm\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import csv \n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f832d5d",
   "metadata": {},
   "source": [
    "# Step 2. 데이터 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "782f92e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Q            A  label\n",
      "0           12시 땡!   하루가 또 가네요.      0\n",
      "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
      "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
      "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
      "4          PPL 심하네   눈살이 찌푸려지죠.      0\n",
      "\n",
      "\n",
      "                 Q            A\n",
      "0           12시 땡!   하루가 또 가네요.\n",
      "1      1지망 학교 떨어졌어    위로해 드립니다.\n",
      "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.\n",
      "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.\n",
      "4          PPL 심하네   눈살이 찌푸려지죠.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# CSV 파일 경로\n",
    "file_path = \"ChatbotData.csv\"  # 파일 경로를 실제 파일 경로로 변경하세요.\n",
    "\n",
    "# CSV 파일 읽기\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 데이터프레임 출력\n",
    "print(df.head())  # 상위 5개 행 출력\n",
    "print(\"\\n\")\n",
    "\n",
    "df = pd.read_csv(file_path, usecols=['Q', 'A'])\n",
    "\n",
    "print(df.head())  # 상위 5개 행 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "036af074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0             12시 땡!\n",
      "1        1지망 학교 떨어졌어\n",
      "2       3박4일 놀러가고 싶다\n",
      "3    3박4일 정도 놀러가고 싶다\n",
      "4            PPL 심하네\n",
      "Name: Q, dtype: object\n",
      "\n",
      "\n",
      "0     하루가 또 가네요.\n",
      "1      위로해 드립니다.\n",
      "2    여행은 언제나 좋죠.\n",
      "3    여행은 언제나 좋죠.\n",
      "4     눈살이 찌푸려지죠.\n",
      "Name: A, dtype: object\n"
     ]
    }
   ],
   "source": [
    "questions =[] \n",
    "answers =[]\n",
    "\n",
    "questions = df['Q']\n",
    "answers = df['A']\n",
    "\n",
    "print(questions[:5])\n",
    "print(\"\\n\")\n",
    "print(answers[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a0a3a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r\"[^a-zA-Zㄱ-ㅎㅏ-ㅣ가-힣0-9,.?!]\", \" \", sentence)\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a4d4b4",
   "metadata": {},
   "source": [
    "# Step 3. 데이터 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3384ceda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "mecab=Mecab()\n",
    "\n",
    "def build_corpus(src, tgt, max_len=50):\n",
    "    \"\"\"\n",
    "    소스 문장과 타겟 문장을 입력으로 받아 전처리, 토큰화, 중복 제거를 수행한 후 반환합니다.\n",
    "    \n",
    "    Args:\n",
    "        src (list): 소스 문장 리스트\n",
    "        tgt (list): 타겟 문장 리스트\n",
    "        max_len (int): 토큰화된 문장의 최대 길이 (기본값 50)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: 토큰화된 소스 문장 리스트와 타겟 문장 리스트\n",
    "    \"\"\"\n",
    "    # 1. 전처리 및 토큰화\n",
    "    corpus = []  # 소스-타겟 쌍을 저장할 리스트\n",
    "    \n",
    "    for src_sentence, tgt_sentence in zip(src, tgt):\n",
    "        # 소스 문장 전처리 및 토큰화\n",
    "        src_tokens = mecab.morphs(preprocess_sentence(src_sentence))\n",
    "        # 타겟 문장 전처리 및 토큰화\n",
    "        tgt_tokens = mecab.morphs(preprocess_sentence(tgt_sentence))\n",
    "        \n",
    "        # 2. 길이 제한 적용\n",
    "        if len(src_tokens) <= max_len and len(tgt_tokens) <= max_len:\n",
    "            src_tokens = ' '.join(src_tokens)  # 공백으로 토큰 결합\n",
    "            tgt_tokens = ' '.join(tgt_tokens)\n",
    "            corpus.append((src_tokens, tgt_tokens))  # 소스-타겟 쌍으로 저장\n",
    "    \n",
    "    # 3. 중복 제거 (쌍 기준으로 중복 제거)\n",
    "    corpus = list(set(corpus))\n",
    "    \n",
    "    # 4. 소스와 타겟 분리\n",
    "    src_corpus, tgt_corpus = zip(*corpus)  # 튜플을 두 개의 리스트로 분리\n",
    "    \n",
    "    return list(src_corpus), list(tgt_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dfed6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Questions: ['헤어진지 두 달 무덤덤 한데', '이 발 어떻게 할까', '만나 는 사람 마다 유형 이 비슷 해', '밥 먹 었 는데 도 또 배고파', '친구 따라다녀야 겠다 .']\n",
      "Tokenized Answers: ['무덤덤 한 데 도 마음 이 허한 가 봅니다 .', '짧 게 변화 를 줘도 괜찮 을 거 같 아요 .', '자신 이 변하 지 않 았 기 때문 이 겠 죠 .', '디 져 트 드세요 .', '친구 가 따라오 게 해 보 세요 .']\n"
     ]
    }
   ],
   "source": [
    "# 함수 실행\n",
    "que_corpus, ans_corpus = build_corpus(questions, answers)\n",
    "\n",
    "# 결과 확인\n",
    "print(\"Tokenized Questions:\", que_corpus[:5])\n",
    "print(\"Tokenized Answers:\", ans_corpus[:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1237406b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11747\n",
      "11747\n"
     ]
    }
   ],
   "source": [
    "print(len(que_corpus))\n",
    "print(len(ans_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44e29b7",
   "metadata": {},
   "source": [
    "# Step 4. Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95bc5c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install gensim==3.8.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2dd157f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec.load('ko.bin')\n",
    "wv = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3d69f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_sub(sentence, word_vectors, threshold=0.7, max_attempts=10):\n",
    "    tokens = sentence.split()\n",
    "    \n",
    "    for _ in range(max_attempts):\n",
    "        selected_tok = random.choice(tokens)\n",
    "        \n",
    "        if selected_tok in word_vectors.vocab:  # 'key_to_index' 대신 'vocab' 사용\n",
    "            similar_words = word_vectors.most_similar(selected_tok, topn=10)\n",
    "            \n",
    "            for word, similarity in similar_words:\n",
    "                if similarity > threshold and word != selected_tok:\n",
    "                    new_sentence = ' '.join([word if tok == selected_tok else tok for tok in tokens])\n",
    "                    return new_sentence\n",
    "    \n",
    "    return None\n",
    "\n",
    "def augment_data(corpus, word_vectors):\n",
    "    augmented_corpus = []\n",
    "    for sentence in tqdm(corpus):\n",
    "        new_sentence = lexical_sub(sentence, word_vectors)\n",
    "        if new_sentence is not None:\n",
    "            augmented_corpus.append(new_sentence)\n",
    "        else:\n",
    "            augmented_corpus.append(sentence)\n",
    "    return augmented_corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9272fc41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11747/11747 [01:51<00:00, 105.49it/s]\n",
      "100%|██████████| 11747/11747 [02:06<00:00, 92.97it/s] \n"
     ]
    }
   ],
   "source": [
    "# 데이터 증강 수행\n",
    "augmented_que_corpus = augment_data(que_corpus, wv)\n",
    "augmented_ans_corpus = augment_data(ans_corpus, wv)\n",
    "\n",
    "# 최종 데이터셋 구성\n",
    "final_que_corpus = que_corpus + augmented_que_corpus + que_corpus\n",
    "final_ans_corpus = ans_corpus + ans_corpus + augmented_ans_corpus\n",
    "\n",
    "# 최종 데이터셋 구성\n",
    "final_dataset = []\n",
    "for i in range(len(que_corpus)):\n",
    "    # 원본 질문 + 원본 답변\n",
    "    final_dataset.append((que_corpus[i], ans_corpus[i]))\n",
    "    \n",
    "    # 증강된 질문 + 원본 답변\n",
    "    final_dataset.append((augmented_que_corpus[i], ans_corpus[i]))\n",
    "    \n",
    "    # 원본 질문 + 증강된 답변\n",
    "    final_dataset.append((que_corpus[i], augmented_ans_corpus[i]))\n",
    "\n",
    "\n",
    "# 데이터 셔플\n",
    "#random.shuffle(final_dataset)\n",
    "\n",
    "# 질문과 답변 분리\n",
    "final_que_corpus, final_ans_corpus = zip(*final_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecd0b589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data size: 11747\n",
      "Augmented data size: 35241\n",
      "Augmentation ratio: 3.0\n",
      "\n",
      "Final dataset (first 15):\n",
      "Q: 헤어진지 두 달 무덤덤 한데\n",
      "A: 무덤덤 한 데 도 마음 이 허한 가 봅니다 .\n",
      "\n",
      "Q: 헤어진지 두 달 무덤덤 한데\n",
      "A: 무덤덤 한 데 도 마음 이 허한 가 봅니다 .\n",
      "\n",
      "Q: 헤어진지 두 달 무덤덤 한데\n",
      "A: 무덤덤 한 데 도 마음 이 허한 가 봅니다 .\n",
      "\n",
      "Q: 이 발 어떻게 할까\n",
      "A: 짧 게 변화 를 줘도 괜찮 을 거 같 아요 .\n",
      "\n",
      "Q: 이 발 어떻게 할까\n",
      "A: 짧 게 변화 를 줘도 괜찮 을 거 같 아요 .\n",
      "\n",
      "Q: 이 발 어떻게 할까\n",
      "A: 짧 게 변화 를 줘도 괜찮 을 거 같 아요 .\n",
      "\n",
      "Q: 만나 는 사람 마다 유형 이 비슷 해\n",
      "A: 자신 이 변하 지 않 았 기 때문 이 겠 죠 .\n",
      "\n",
      "Q: 만나 는 사람 마다 유형 이 유사 해\n",
      "A: 자신 이 변하 지 않 았 기 때문 이 겠 죠 .\n",
      "\n",
      "Q: 만나 는 사람 마다 유형 이 비슷 해\n",
      "A: 자신 이 바뀌 지 않 았 기 때문 이 겠 죠 .\n",
      "\n",
      "Q: 밥 먹 었 는데 도 또 배고파\n",
      "A: 디 져 트 드세요 .\n",
      "\n",
      "Q: 밥 먹 었 으며 도 또 배고파\n",
      "A: 디 져 트 드세요 .\n",
      "\n",
      "Q: 밥 먹 었 는데 도 또 배고파\n",
      "A: 디 져 트 드세요 .\n",
      "\n",
      "Q: 친구 따라다녀야 겠다 .\n",
      "A: 친구 가 따라오 게 해 보 세요 .\n",
      "\n",
      "Q: 연인 따라다녀야 겠다 .\n",
      "A: 친구 가 따라오 게 해 보 세요 .\n",
      "\n",
      "Q: 친구 따라다녀야 겠다 .\n",
      "A: 친구 가 찾아오 게 해 보 세요 .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Original data size:\", len(que_corpus))\n",
    "print(\"Augmented data size:\", len(final_que_corpus))\n",
    "print(\"Augmentation ratio:\", len(final_que_corpus) / len(que_corpus))\n",
    "\n",
    "# 결과 확인\n",
    "print(\"\\nFinal dataset (first 15):\")\n",
    "for i in range(15):\n",
    "    print(f\"Q: {final_que_corpus[i]}\")\n",
    "    print(f\"A: {final_ans_corpus[i]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730afcc9",
   "metadata": {},
   "source": [
    "# Step 5. 데이터 벡터화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea69bf60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 사전 크기: 7373\n",
      "enc_train shape: (35241, 32)\n",
      "dec_train shape: (35241, 46)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "# 1. 타겟 데이터에 <start>와 <end> 토큰 추가\n",
    "final_ans_corpus = [['<start>'] + answer + ['<end>'] for answer in final_ans_corpus]\n",
    "\n",
    "# 전체 코퍼스 생성 (질문과 답변 결합)\n",
    "total_corpus = list(final_que_corpus) + [' '.join(answer) for answer in final_ans_corpus]\n",
    "\n",
    "# 2. 단어 사전 구축 및 벡터화\n",
    "tokenizer = Tokenizer(filters='', oov_token='OOV')\n",
    "tokenizer.fit_on_texts(total_corpus)\n",
    "\n",
    "# 질문(인코더 입력) 벡터화\n",
    "enc_train = tokenizer.texts_to_sequences(final_que_corpus)\n",
    "enc_train = pad_sequences(enc_train, padding='post')\n",
    "\n",
    "# 답변(디코더 입력) 벡터화\n",
    "dec_train = tokenizer.texts_to_sequences([' '.join(answer) for answer in final_ans_corpus])\n",
    "dec_train = pad_sequences(dec_train, padding='post')\n",
    "\n",
    "print(\"단어 사전 크기:\", len(tokenizer.word_index))\n",
    "print(\"enc_train shape:\", enc_train.shape)\n",
    "print(\"dec_train shape:\", dec_train.shape)\n",
    "\n",
    "\n",
    "# 단어 사전 저장 (선택사항)\n",
    "import pickle\n",
    "with open('word_index.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer.word_index, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5760bb",
   "metadata": {},
   "source": [
    "# Step 6. 훈련하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e44a78",
   "metadata": {},
   "source": [
    "## 트랜스포머 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a24e52b",
   "metadata": {},
   "source": [
    "### positional_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41c8051a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, (2*(i//2)) / np.float32(d_model))\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "\n",
    "    return sinusoid_table\n",
    "    \n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8944ba",
   "metadata": {},
   "source": [
    "### 마스크 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b8148719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_lookahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_enc_mask = generate_padding_mask(src)\n",
    "\n",
    "    dec_lookahead_mask = generate_lookahead_mask(tgt.shape[1])\n",
    "    dec_tgt_padding_mask = generate_padding_mask(tgt)\n",
    "    dec_mask = tf.maximum(dec_tgt_padding_mask, dec_lookahead_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask\n",
    "    \n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9c9713",
   "metadata": {},
   "source": [
    "### MultiHeadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1046a5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "\n",
    "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None: scaled_qk += (mask * -1e9)  \n",
    "\n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "        out = tf.matmul(attentions, V)\n",
    "\n",
    "        return out, attentions\n",
    "        \n",
    "\n",
    "    def split_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        split_x = tf.reshape(x, (bsz, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
    "\n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (bsz, -1, self.d_model))\n",
    "\n",
    "        return combined_x\n",
    "\n",
    "    \n",
    "    def call(self, Q, K, V, mask):\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "        \n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "        \n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask)\n",
    "                        \n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "            \n",
    "        return out, attention_weights\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dae3ee",
   "metadata": {},
   "source": [
    "### Position-wise Feed Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3830a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# Position-wise Feed Forward Network 구현\n",
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        self.fc1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.fc2(out)\n",
    "            \n",
    "        return out\n",
    "        \n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f8f13f",
   "metadata": {},
   "source": [
    "### EncoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "51a72554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# Encoder의 레이어 구현\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        '''\n",
    "        Multi-Head Attention\n",
    "        '''\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        '''\n",
    "        Position-Wise Feed Forward Network\n",
    "        '''\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        return out, enc_attn\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def2dc9f",
   "metadata": {},
   "source": [
    "### DecoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "55581e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# Decoder 레이어 구현\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "    \n",
    "    def call(self, x, enc_out, dec_enc_mask, padding_mask):\n",
    "        '''\n",
    "        Masked Multi-Head Attention\n",
    "        '''\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        '''\n",
    "        Multi-Head Attention\n",
    "        '''\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        # Q, K, V 순서에 주의하세요!\n",
    "        out, dec_enc_attn = self.enc_dec_attn(Q=out, K=enc_out, V=enc_out, mask=dec_enc_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        '''\n",
    "        Position-Wise Feed Forward Network\n",
    "        '''\n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca43851",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "01716f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# Encoder 구현\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                        for _ in range(n_layers)]\n",
    "    \n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "    \n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "        \n",
    "        return out, enc_attns\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b11a1f4",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "03404f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# Decoder 구현\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                            for _ in range(n_layers)]\n",
    "                            \n",
    "    def call(self, x, enc_out, dec_enc_mask, padding_mask):\n",
    "        out = x\n",
    "    \n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = \\\n",
    "            self.dec_layers[i](out, enc_out, dec_enc_mask, padding_mask)\n",
    "\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "\n",
    "        return out, dec_attns, dec_enc_attns\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db54f6e7",
   "metadata": {},
   "source": [
    "### Transformer Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d14e53ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 n_layers,\n",
    "                 d_model,\n",
    "                 n_heads,\n",
    "                 d_ff,\n",
    "                 src_vocab_size,\n",
    "                 tgt_vocab_size,\n",
    "                 pos_len,\n",
    "                 dropout=0.2,\n",
    "                 shared_fc=True,\n",
    "                 shared_emb=False):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        # 초기화 파라미터 저장\n",
    "        self.n_layers = n_layers\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.n_heads = n_heads\n",
    "        self.d_ff = d_ff\n",
    "        self.dropout_rate = dropout\n",
    "        \n",
    "        if shared_emb:\n",
    "            self.enc_emb = self.dec_emb = \\\n",
    "            tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        else:\n",
    "            self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "            self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "\n",
    "        self.shared_fc = shared_fc\n",
    "\n",
    "        if shared_fc:\n",
    "            self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "\n",
    "    def embedding(self, emb, x):\n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        out = emb(x)\n",
    "\n",
    "        if self.shared_fc: out *= tf.math.sqrt(self.d_model)\n",
    "\n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.do(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "        \n",
    "    def call(self, enc_in, dec_in, enc_mask, dec_enc_mask, dec_mask):\n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "        \n",
    "        dec_out, dec_attns, dec_enc_attns = \\\n",
    "        self.decoder(dec_in, enc_out, dec_enc_mask, dec_mask)\n",
    "        \n",
    "        logits = self.fc(dec_out)\n",
    "        \n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ef1ec53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(tokenizer.word_index) + 1  # tokenizer에서 vocab size를 가져옵니다\n",
    "MAX_LEN = max(enc_train.shape[1], dec_train.shape[1])  # 최대 시퀀스 길이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "61735af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 주어진 하이퍼파라미터로 Transformer 인스턴스 생성\n",
    "\n",
    "transformer = Transformer(\n",
    "    n_layers=1,\n",
    "    d_model=368,\n",
    "    n_heads=8,\n",
    "    d_ff=1024,\n",
    "    src_vocab_size=VOCAB_SIZE,\n",
    "    tgt_vocab_size=VOCAB_SIZE,\n",
    "    pos_len=200,\n",
    "    dropout=0.2,\n",
    "    shared_fc=True,\n",
    "    shared_emb=True)\n",
    "\t\t\n",
    "d_model = 368\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "db907d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# Learning Rate Scheduler 구현\n",
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=1000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "74dc2d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# Learning Rate 인스턴스 선언 & Optimizer 구현\n",
    "\n",
    "\n",
    "learning_rate = LearningRateScheduler(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
    "                                        beta_1=0.9,\n",
    "                                        beta_2=0.98, \n",
    "                                        epsilon=1e-9)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "deb69555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# Loss Function 정의\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b4a363bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# Train Step 정의\n",
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    tgt_in = tgt[:, :-1]  # Decoder의 input\n",
    "    gold = tgt[:, 1:]     # Decoder의 output과 비교하기 위해 right shift를 통해 생성한 최종 타겟\n",
    "\n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        model(src, tgt_in, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45962970",
   "metadata": {},
   "source": [
    "## 훈련하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "638fb3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "551it [00:28, 19.62it/s, Epoch=1/10, Loss=1.6048]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Loss: 2.8268\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "551it [00:22, 24.53it/s, Epoch=2/10, Loss=1.0318]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 Loss: 1.1765\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "551it [00:22, 24.41it/s, Epoch=3/10, Loss=0.4144]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 Loss: 0.6733\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "551it [00:23, 23.88it/s, Epoch=4/10, Loss=0.3662]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 Loss: 0.3842\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "551it [00:23, 23.48it/s, Epoch=5/10, Loss=0.2622]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 Loss: 0.2675\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "551it [00:23, 23.22it/s, Epoch=6/10, Loss=0.2017]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 Loss: 0.2089\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "551it [00:23, 23.13it/s, Epoch=7/10, Loss=0.2067]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7 Loss: 0.1743\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "551it [00:24, 22.76it/s, Epoch=8/10, Loss=0.1204]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 Loss: 0.1512\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "551it [00:24, 22.94it/s, Epoch=9/10, Loss=0.1008]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9 Loss: 0.1327\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "551it [00:24, 22.70it/s, Epoch=10/10, Loss=0.1120]                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10 Loss: 0.1212\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Q. 위의 코드를 활용하여 모델을 훈련시켜봅시다!\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "DATASET_SIZE = len(enc_train)  # 전체 데이터셋 크기를 미리 계산\n",
    "\n",
    "# 데이터셋 생성\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train))\n",
    "train_dataset = train_dataset.shuffle(DATASET_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "STEPS_PER_EPOCH = DATASET_SIZE // BATCH_SIZE  # 에폭당 스텝 수 계산\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    tqdm_bar = tqdm(total=STEPS_PER_EPOCH)  # 스텝 수를 tqdm에 전달\n",
    "    \n",
    "    for (batch, (src, tgt)) in enumerate(train_dataset):\n",
    "        batch_loss, _, _, _ = train_step(src, tgt, transformer, optimizer)\n",
    "        total_loss += batch_loss\n",
    "        tqdm_bar.update(1)\n",
    "        tqdm_bar.set_postfix({\n",
    "            'Epoch': f'{epoch+1}/{EPOCHS}',\n",
    "            'Loss': f'{batch_loss.numpy():.4f}'\n",
    "        })\n",
    "    \n",
    "    tqdm_bar.close()  # tqdm 객체 닫기\n",
    "    \n",
    "    epoch_loss = total_loss / STEPS_PER_EPOCH\n",
    "    print(f'\\nEpoch {epoch+1} Loss: {epoch_loss:.4f}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a1aa59",
   "metadata": {},
   "source": [
    "## 챗봇 대답 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "30e66316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def translate(sentence, model, tokenizer):\n",
    "    tokens = tokenizer.texts_to_sequences([sentence])[0]\n",
    "    padded_tokens = tf.keras.preprocessing.sequence.pad_sequences([tokens],\n",
    "                                                           maxlen=MAX_LEN,\n",
    "                                                           padding='post')\n",
    "    output = tf.expand_dims([tokenizer.word_index['<start>']], 0)   \n",
    "    for i in range(MAX_LEN):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
    "        generate_masks(padded_tokens, output)\n",
    "\n",
    "        predictions, _, _, _ = model(padded_tokens, \n",
    "                                     output,\n",
    "                                     enc_padding_mask,\n",
    "                                     combined_mask,\n",
    "                                     dec_padding_mask)\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[:, -1:, :], axis=-1).numpy()[0][0]\n",
    "\n",
    "        if tokenizer.index_word[predicted_id] == '<end>':\n",
    "            break\n",
    "\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "\n",
    "    # <start> 토큰을 제외하고 결과 생성\n",
    "    result = ' '.join([tokenizer.index_word[i] for i in output.numpy()[0][1:] if i not in [0, tokenizer.word_index['<start>']]])\n",
    "    \n",
    "    # 마지막에 <end> 토큰 추가\n",
    "    result = result + ' <end>'\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ad567940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 제출\n",
      "\n",
      "Translations\n",
      "> 1. 이제 보 지 마세요 . <end>\n",
      "> 2. 오늘 도 고생 이 많 았 어요 . <end>\n",
      "> 3. 오래 살 면 정말 떨리 죠 . <end>\n",
      "> 4. 이제 보 지 마세요 . <end>\n",
      "\n",
      "Hyperparameters\n",
      "> n_layers: 1\n",
      "> d_model: 368.0\n",
      "> n_heads: 8\n",
      "> d_ff: 1024\n",
      "> dropout: 0.2\n",
      "\n",
      "Training Parameters\n",
      "> Warmup Steps: 1000\n",
      "> Batch Size: 64\n",
      "> Epoch At: 10\n"
     ]
    }
   ],
   "source": [
    "# 예문 번역 및 결과 출력\n",
    "examples = [\n",
    "    \"지루하다, 놀러가고 싶어.\",\n",
    "    \"오늘 일찍 일어났더니 피곤하다.\",\n",
    "    \"간만에 여자친구랑 데이트 하기로 했어.\",\n",
    "    \"집에 있는다는 소리야.\"\n",
    "]\n",
    "\n",
    "print(\"# 제출\\n\")\n",
    "print(\"Translations\")\n",
    "for i, example in enumerate(examples, 1):\n",
    "    translation = translate(example, transformer, tokenizer)\n",
    "    print(f\"> {i}. {translation}\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nHyperparameters\")\n",
    "print(f\"> n_layers: {transformer.n_layers}\")\n",
    "print(f\"> d_model: {transformer.d_model.numpy()}\")\n",
    "print(f\"> n_heads: {transformer.n_heads}\")\n",
    "print(f\"> d_ff: {transformer.d_ff}\")\n",
    "print(f\"> dropout: {transformer.dropout_rate}\")\n",
    "\n",
    "print(\"\\nTraining Parameters\")\n",
    "print(f\"> Warmup Steps: {learning_rate.warmup_steps}\")\n",
    "print(f\"> Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"> Epoch At: {EPOCHS}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0063e84d",
   "metadata": {},
   "source": [
    "# Step 7. 성능 측정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8da8a588",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def calculate_bleu(reference, candidate):\n",
    "    # Mecab으로 토큰화\n",
    "    ref_tokens = mecab.morphs(reference)\n",
    "    can_tokens = mecab.morphs(candidate)\n",
    "    \n",
    "    return sentence_bleu([ref_tokens],\n",
    "                         can_tokens,\n",
    "                         smoothing_function=SmoothingFunction().method1)\n",
    "\n",
    "\n",
    "def eval_bleu(model, samples, tokenizer):\n",
    "    total_score = 0.0\n",
    "    sample_size = len(samples)\n",
    "    \n",
    "    print(\"Translations and BLEU Scores\")\n",
    "    for idx, (question, answer) in enumerate(samples, 1):\n",
    "        translation = translate(question, model, tokenizer)\n",
    "        \n",
    "        # 원문 및 결과 토큰화 출력\n",
    "        ref_tokens = mecab.morphs(answer)\n",
    "        can_tokens = mecab.morphs(translation)\n",
    "        \n",
    "        bleu_score = calculate_bleu(answer, translation)\n",
    "        total_score += bleu_score\n",
    "        \n",
    "        print(f\"\\n> {idx}. Question: {question}\")\n",
    "        print(f\"   Original Answer: {answer}\")\n",
    "        print(f\"   Original Tokens: {ref_tokens}\")  # 토큰화 결과 출력\n",
    "        print(f\"   Model Translation: {translation}\")\n",
    "        print(f\"   Model Tokens: {can_tokens}\")  # 토큰화 결과 출력\n",
    "        print(f\"   BLEU Score: {bleu_score:.4f}\")\n",
    "    \n",
    "    avg_bleu_score = total_score / sample_size\n",
    "    print(f\"\\nNumber of Samples: {sample_size}\")\n",
    "    print(f\"Average BLEU Score: {avg_bleu_score:.4f}\")\n",
    "    \n",
    "    return avg_bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "16ca4027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BLEU Score Evaluation\n",
      "Translations and BLEU Scores\n",
      "\n",
      "> 1. Question: 좋아하는 여자가 다른 남자랑 친해지는게 짜증남.\n",
      "   Original Answer: 충분히 신경쓰인 부분이에요.\n",
      "   Original Tokens: ['충분히', '신경', '쓰인', '부분', '이', '에요', '.']\n",
      "   Model Translation: 때 의 마음 이 깊 네요 . <end>\n",
      "   Model Tokens: ['때', '의', '마음', '이', '깊', '네요', '.', '<', 'end', '>']\n",
      "   BLEU Score: 0.0251\n",
      "\n",
      "> 2. Question: 원래 다 이런거겠죠\n",
      "   Original Answer: 누구나 그럴 거예요.\n",
      "   Original Tokens: ['누구', '나', '그럴', '거', '예요', '.']\n",
      "   Model Translation: 누구 나 한테 도 잊 으세요 . <end>\n",
      "   Model Tokens: ['누구', '나', '한테', '도', '잊', '으세요', '.', '<', 'end', '>']\n",
      "   BLEU Score: 0.0494\n",
      "\n",
      "> 3. Question: 맨날 똑같애\n",
      "   Original Answer: 똑같은 건 없어요.\n",
      "   Original Tokens: ['똑같', '은', '건', '없', '어요', '.']\n",
      "   Model Translation: 때론 묻어두 는 게 좋 을 때 도 있 어요 . <end>\n",
      "   Model Tokens: ['때론', '묻어두', '는', '게', '좋', '을', '때', '도', '있', '어요', '.', '<', 'end', '>']\n",
      "   BLEU Score: 0.0302\n",
      "\n",
      "> 4. Question: 입소문이 무서워\n",
      "   Original Answer: 긍정적이고 부정적인 파급력이 모두 세죠.\n",
      "   Original Tokens: ['긍정', '적', '이', '고', '부정', '적', '인', '파급', '력', '이', '모두', '세', '죠', '.']\n",
      "   Model Translation: 긍정 적 이 고 부정 적 인 파급 력 이 모두 세 죠 . <end>\n",
      "   Model Tokens: ['긍정', '적', '이', '고', '부정', '적', '인', '파급', '력', '이', '모두', '세', '죠', '.', '<', 'end', '>']\n",
      "   BLEU Score: 0.8053\n",
      "\n",
      "> 5. Question: 일전에 여기서 댓글주고 받던분들.\n",
      "   Original Answer: 같은 고민을 안고 살아가는 사람들.\n",
      "   Original Tokens: ['같', '은', '고민', '을', '안', '고', '살아가', '는', '사람', '들', '.']\n",
      "   Model Translation: 처음 이 ㄴ가요 . <end>\n",
      "   Model Tokens: ['처음', '이', 'ㄴ', '가', '요', '.', '<', 'end', '>']\n",
      "   BLEU Score: 0.0192\n",
      "\n",
      "> 6. Question: 이건 아파도 너무 아프잖아.\n",
      "   Original Answer: 예상치 못한 이별에 면역이 덜 되었나봐요.\n",
      "   Original Tokens: ['예상', '치', '못한', '이별', '에', '면역', '이', '덜', '되', '었', '나', '봐요', '.']\n",
      "   Model Translation: 예상 치 못한 이별 에 면역 이 덜 어 었 나 봐요 . <end>\n",
      "   Model Tokens: ['예상', '치', '못한', '이별', '에', '면역', '이', '덜', '어', '었', '나', '봐요', '.', '<', 'end', '>']\n",
      "   BLEU Score: 0.6026\n",
      "\n",
      "> 7. Question: 클럽에서 만나서 사귀는거 어떻게 생각해?\n",
      "   Original Answer: 어디서 만났든 그 후가 중요해요.\n",
      "   Original Tokens: ['어디', '서', '만났', '든', '그', '후', '가', '중요', '해요', '.']\n",
      "   Model Translation: 이제 보 지 마세요 . <end>\n",
      "   Model Tokens: ['이제', '보', '지', '마세요', '.', '<', 'end', '>']\n",
      "   BLEU Score: 0.0216\n",
      "\n",
      "> 8. Question: 경쟁이 너무 치열해\n",
      "   Original Answer: 점점 치열해지는 것 같아요.\n",
      "   Original Tokens: ['점점', '치열', '해', '지', '는', '것', '같', '아요', '.']\n",
      "   Model Translation: 이제 보 지 마세요 . <end>\n",
      "   Model Tokens: ['이제', '보', '지', '마세요', '.', '<', 'end', '>']\n",
      "   BLEU Score: 0.0292\n",
      "\n",
      "> 9. Question: 여행갈 시간도 없다\n",
      "   Original Answer: 바쁘게 사나봅니다.\n",
      "   Original Tokens: ['바쁘', '게', '사', '나', '봅니다', '.']\n",
      "   Model Translation: 이제 보 지 마세요 . <end>\n",
      "   Model Tokens: ['이제', '보', '지', '마세요', '.', '<', 'end', '>']\n",
      "   BLEU Score: 0.0278\n",
      "\n",
      "> 10. Question: 친구 결혼식에서 축가 준비 중\n",
      "   Original Answer: 친구가 좋아하겠어요.\n",
      "   Original Tokens: ['친구', '가', '좋', '아', '하', '겠', '어요', '.']\n",
      "   Model Translation: 친구 가 좋 아 하 겠 네요 . <end>\n",
      "   Model Tokens: ['친구', '가', '좋', '아', '하', '겠', '네요', '.', '<', 'end', '>']\n",
      "   BLEU Score: 0.4799\n",
      "\n",
      "Number of Samples: 10\n",
      "Average BLEU Score: 0.2090\n",
      "\n",
      "Final Average BLEU Score: 0.2090\n",
      "\n",
      "Hyperparameters\n",
      "> n_layers: 1\n",
      "> d_model: 368.0\n",
      "> n_heads: 8\n",
      "> d_ff: 1024\n",
      "> dropout: 0.2\n",
      "\n",
      "Training Parameters\n",
      "> Warmup Steps: 1000\n",
      "> Batch Size: 64\n",
      "> Epoch At: 10\n"
     ]
    }
   ],
   "source": [
    "# 무작위로 샘플 선택\n",
    "num_samples = 10  # 선택할 샘플 수\n",
    "samples = df.sample(n=num_samples)[['Q', 'A']].values.tolist()\n",
    "\n",
    "# BLEU 점수 계산\n",
    "print(\"\\nBLEU Score Evaluation\")\n",
    "avg_bleu = eval_bleu(transformer, samples, tokenizer)\n",
    "\n",
    "print(f\"\\nFinal Average BLEU Score: {avg_bleu:.4f}\")\n",
    "\n",
    "print(\"\\nHyperparameters\")\n",
    "print(f\"> n_layers: {transformer.n_layers}\")\n",
    "print(f\"> d_model: {transformer.d_model.numpy()}\")\n",
    "print(f\"> n_heads: {transformer.n_heads}\")\n",
    "print(f\"> d_ff: {transformer.d_ff}\")\n",
    "print(f\"> dropout: {transformer.dropout_rate}\")\n",
    "\n",
    "print(\"\\nTraining Parameters\")\n",
    "print(f\"> Warmup Steps: {learning_rate.warmup_steps}\")\n",
    "print(f\"> Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"> Epoch At: {EPOCHS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2cdbc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb90c42e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be67955",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e22108",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77543c4b",
   "metadata": {},
   "source": [
    "# 회고"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c63e603",
   "metadata": {},
   "source": [
    "4. Question: 입소문이 무서워\n",
    "   Original Answer: 긍정적이고 부정적인 파급력이 모두 세죠.\n",
    "   Original Tokens: ['긍정', '적', '이', '고', '부정', '적', '인', '파급', '력', '이', '모두', '세', '죠', '.']\n",
    "   Model Translation: 긍정 적 이 고 부정 적 인 파급 력 이 모두 세 죠 . <end>\n",
    "   Model Tokens: ['긍정', '적', '이', '고', '부정', '적', '인', '파급', '력', '이', '모두', '세', '죠', '.', '<', 'end', '>']\n",
    "   BLEU Score: 0.8053"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db646bf5",
   "metadata": {},
   "source": [
    "<end> 토큰 처리 후 다시 점수를 계산해야할듯,,!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42cff6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
